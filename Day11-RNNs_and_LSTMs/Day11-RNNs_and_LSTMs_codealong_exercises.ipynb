{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hqjd41KIyXK"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1DvKhAzLtk-Hilu7Le73WAOz2EBR5d41G\" width=\"500\"/>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NjC_V_MhWkF"
   },
   "source": [
    "# RNNs and LSTMs (Solutions):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWEtzHPxiPL4"
   },
   "source": [
    "#### A few imports and functions before we get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3trt71yj33T",
    "outputId": "39ece5e2-1e1a-47a7-d719-6a410831792c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycm in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (7.5.4)\n",
      "Requirement already satisfied: livelossplot in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (0.5.5)\n",
      "Requirement already satisfied: matplotlib in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from livelossplot) (3.7.2)\n",
      "Requirement already satisfied: bokeh in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from livelossplot) (3.2.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from bokeh->livelossplot) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from bokeh->livelossplot) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from bokeh->livelossplot) (1.24.3)\n",
      "Requirement already satisfied: packaging>=16.8 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from bokeh->livelossplot) (23.1)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from bokeh->livelossplot) (2.0.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from bokeh->livelossplot) (9.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from bokeh->livelossplot) (6.0)\n",
      "Requirement already satisfied: tornado>=5.1 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from bokeh->livelossplot) (6.3.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from bokeh->livelossplot) (2022.9.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from matplotlib->livelossplot) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from matplotlib->livelossplot) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from matplotlib->livelossplot) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from matplotlib->livelossplot) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from matplotlib->livelossplot) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from pandas>=1.2->bokeh->livelossplot) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from pandas>=1.2->bokeh->livelossplot) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/javonne/anaconda3/envs/mlds4p/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "!pip install pycm livelossplot\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IDJIvzXKhgLC",
    "outputId": "d7a83107-800c-4fe0-bc20-09d6cb2ee639"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available! Running on CPU\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import glob\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  # uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    print(torch.cuda.get_device_name())\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available! Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFDN7Z9kyLKZ"
   },
   "source": [
    "#### Mounting the google drive for storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKPR7ozyyKWo",
    "outputId": "3fc46dad-46b3-493a-8482-95a5bb2303f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIP-LJAk5Lnv"
   },
   "source": [
    "## Word-level text generation with RNNs\n",
    "\n",
    "\n",
    "[Let's see if you can differentiate between machine generated text and human written text.](http://goopt2.xyz/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtkDguqVnDzv"
   },
   "source": [
    "We will use RNNs to build our generator network but you can also consider using LSTMs, which have a gating mechanism that allows information to continue flowing into the layers and cells of the network and have been showed to outperform vanilla RNNs for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyFH5cWIT7xM"
   },
   "source": [
    "### Downloading the data and some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cuq8un0T-6c",
    "outputId": "9e2d4b35-a003-4d6b-a0d4-82dda0958052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data_gen’: File exists\n",
      "--2023-12-12 14:30:15--  https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 141847 (139K) [text/plain]\n",
      "Saving to: ‘reddit-cleanjokes.csv.1’\n",
      "\n",
      "reddit-cleanjokes.c 100%[===================>] 138.52K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-12-12 14:30:15 (3.21 MB/s) - ‘reddit-cleanjokes.csv.1’ saved [141847/141847]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the data\n",
    "!mkdir data_gen\n",
    "!cd data_gen && wget https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv\n",
    "\n",
    "filename = 'data_gen/reddit-cleanjokes.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlGwjh4LchS1"
   },
   "source": [
    "#### Function to read the lines of a file as a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5WF2Qjx5Vzx",
    "outputId": "08ed7a93-50d1-4f92-b4ec-5a247b4948ee"
   },
   "outputs": [],
   "source": [
    "def readFile_csv(filename, header):\n",
    "    \"\"\"\n",
    "    Read a csv file and return list with line entries\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(filename)\n",
    "    data = dataframe[header].str.cat(sep=' ')\n",
    "    data = data.split(' ')\n",
    "    return data\n",
    "\n",
    "filename, header = 'data_gen/reddit-cleanjokes.csv', 'Joke'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il8s4ifodIpT"
   },
   "source": [
    "#### Data inspection and some more utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5PhEj2Advnn"
   },
   "source": [
    "We use a counter from the collections module to create a dictionary where the words are stored as the keys and their counts are the values. You can read more about collections [here](https://docs.python.org/3/library/collections.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U16w5wMwDgdM",
    "outputId": "3260c319-1bcd-43f4-fd47-ca5c285cc383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23914\n",
      "{'', 'feeling', 'Theresa', 'cry,', 'out.', 'cloud', 'slow', 'degrees.', 'leg.', 'bartender.', 'closer', 'Asian', 'rash?', 'free', 'Humblebee.', 'Thunderwear!', 'skip', 'sink', 'me:', 'Mercury...', 'grey?', 'THERE!', 'Humpty', 'reception', 'constipated', '**Person', 'attire.', 'trunk.', 'citizens', 'proofing', '&gt;Nacho', 'Some', 'ceiling', 'parts', 'prefer', 'alright', 'Lucifer', 'At', 'locomotives?', 'murder?', 'gas', 'themselves.', 'even?', 'left', \"victim's\", 'scallion.', 'MARSHian', 'say...', 'stick!', 'meat?', 'make?', 'add', 'person', '\"I\\'ll', '**Tim', 'winter?', 'knock*', 'bunny', 'shows', 'boomer-WRONG!', 'bucket~~', 'tuba', 'Shanghai.', 'forty', 'else', 'expert', 'eight', 'inches', 'Sauron', 'home?', 'Ice', 'my-oh-my!', 'blind', 'shape.', 'mop...\"', 'feet?', 'goatherd', 'direction', 'hole', '9', \"Picard's\", 'bought', 'bars', 'JaPAN!', 'clauset.', '**Jimmy', 'Three', 'like?', 'MOOOOOOOOOOOOOOOO!!!!', '\"Breathe,', '\"p\"', 'Up', 'Another', 'page.\"', 'highly', 'you?', 'spied', 'paper.', 'hurt', '^voice', 'roundest', 'grabbing?', 'bones', 'record', 'Flies!', 'month', 'fluff', 'hopefully', 'Bloom.', 'chase', 'tissue', \"Wawa's\", 'priest,', 'As', 'warden', 'burial', 'atoms', 'Lee', 'Lane.', 'buying', '_Even', \"hasn't\", 'u/BostonCentrist', 'incorrect\",', 'duh', 'Bob', 'toad', 'water', 'situation', 'true', 'numbered.', 'said,', 'original', '3)', 'chillies', 'Beep', 'overweight?', 'flag', 'wear?', 'parents?', 'show...', 'monkey?', 'plains', 'booger', 'You:', 'Egyptians', 'circus?', 'Wash', 'curse', 'self!', 'now?', 'Miss', 'matter?', 'bell?', 'Congee-gal', 'brownie', 'Happy', 'jerks.', 'survivors', 'cook', 'scare?', '(Indiana)', 'sodium.', 'hotdog?', 'koala', 'Table?', 'together?', \"dog's\", 'Baroque.', '...while', 'OPPORTUNITY', \"Ahab's\", 'soup', 'An', 'thing', 'Soap', '97', 'birds', 'horn-less', \"Lee's\", 'Unfortunately,', 'Novocain?', 'bowl', 'bath', 'understand', 'how', 'fuzzy,', 'up', 'iPhones', 'longer', 'stop', 'moving', 'composers?', 'older', 'pew.', 'before', 'Jew', 'flatman', 'probate', 'tribesman', 'Frenchman', 'today?\"', 'tainted', 'shoot', 'Which', 'system?', 'townspeople', 'cactussssssssssssssssssss!', 'soda-pressed.', 'fingers.', 'she', 'dedication.', 'black,', 'wait..', 'late', 'after!', 'fire?', 'juggler.', '\"Keep', 'stick.', 'Alrighty', 'healthier...', 'Way\".', 'radical', 'milkshake', 'disappointed;', 'to', 'Woohoo!', 'frogs', '--My', '\"Your', 'Branson', 'here\"', 'TV,', 'guys!', 'us.', 'Bar](http://www.gettyimages.com/detail/photo/friends-at-cafe-bar-high-res-stock-photography/156534295)', 'solve', 'kinda', 'says:', 'coop', 'sticky?', 'Bop!', 'techromancer.', 'seven', 'literal', 'game', 'grandpa', 'guys/gals', 'serve', 'major', '1:How', 'irony?', '...he', 'autumnomy', 'Catholic.', 'plays', 'photos?', 'flat', 'denims', 'orbits', 'cops.', 'leapfrog?', 'stern', 'Yeast', 'Limp', '\"purple\"?', 'emotional', 'COUS!!!\"', 'Person', '\"Marshmallow.\"', 'babies!', 'fruits?', 'minister,', 'hill.', 'dysfunction.', 'clark', '&amp;', 'channel', 'Hurty!', 'dune', 'strapless.', 'stallions', 'him.', 'camel', 'ocean', 'papa', 'Polish.', 'sheep-human', 'Cafe', 'ends?', 'help.', 'You', 'toe?', 'fruit-picker', 'grow', 'sex', 'pride', 'Paris.', 'country?', 'committed', 'covers', 'fact,', 'armed.', 'heads', 'bathroom', 'Java', 'prints.', 'Whalers!', 'two-tired...', 'Okay,', 'robot', 'Thought', 'Flabio.', 'here,', 'calmed', 'fractions?', 'high', 'Two', 'something.', 'extraterrestrial', 'morning.', 'media', 'Cantelope', 'wurst!', 'bananas!', 'reason.', 'now,', 'bed?', 'Phil', 'theme', 'Delaware?', \"drift?''\", 'storm?', 'financial', 'dropped', 'Someone', \"father's\", 'aircraft.', 'dinosaurs', 'TAHH!!!!', 'calculating', 'Choose', 'lengthy', 'cats,', 'Bond', 'shear', 'weakness', 'senior', 'Orlando', 'it...', 'planes', 'shoulders', 'boxer?', 'Velcro', 'drizzle,', 'Changing', 'airport', 'went', 'Kind', 'handle', 'pin', 'phone', 'microscope?', 'loaded!', 'answers,', 'education?\"', 'throws', 'Hillary', 'Motives', 'ends', 'mass?', 'though,', '(when', 'Business', 'leek?', 'Gerald', 'work?', 'neighbor', 'filling!', 'ATM...', 'G', 'Marios', 'lid', \"'em\", 'opens', 'Urine', 'funding.', 'nowhere.\"', 'rain', 'Parrot?', 'zombies', 'Nothing', 'feet.', 'scheme.', 'trained.', 'Wrights', 'Fe.', 'tss!', 'stoned', 'drivers?', 'near', 'prostitute', 'time,', \"won't\", 'art', 'pachyderm', 'times?', 'travelling', 'Frozone', 'ate', 'shops', 'genealogy', 'comedian.', 'voted', 'Snoop', 'Day:', 'WURST.', 'within', 'options.', 'see', '...wiping!!!\"', '3', 'mittens', 'light?', 'architects', 'night?', 'under', 'star', 'foot!', 'do?', 'novelist', 'apart', '\"ooh', 'show?', 'Doublemint,', 'crime', 'Monopoly.', 'surprised.', 'table', 'seafood', 'fat', 'desk?', 'worth', 'lather', 'Spanish', 'between', 'tie.', 'Curry-er.', 'Boop', 'competition...', 'got,', 'them...', 'Abby.', 'shoplifting', 'bury', 'die', 'film', 'waitress', 'remind', 'beef', 'ultimate', 'signature', 'charge', 'moon?', 'Edit:', 'this?', 'sioux', 'halitosis.', 'exchange', 'stupid!\"', 'soft', 'duck?', 'favourite', 'veteran.', 'sandy', 'seaweed', 'fail', 'barcode...', 'Cash.', 'fudge', 'OTHER', 'Out', 'sew.\"', 'better.', 'bicycles?', 'seem', 'impersonated', 'steamroller?', 'Jaundice', 'can!', ':Yes,', 'Wales', 'fuzzy', 'alphabet?', 'dispenser.', 'http://dryinginside.blogspot.com/2012/10/reinventing-yourself-doesnt-always-work.html', 'melons', 'Shear', 'relative?', 'fit!', 'idea.', 'rock', 'pork', 'passion', 'Pro-Bono', 'obvious?', 'mods', 'pf', 'two', '\"assaulted\"', 'Galaxy.', 'Wild', 'two-tired', '*tale*', 'been?', 'tiles', 'cows', 'job?', 'plumber', 'leery', 'illegal', 'worst', 'ball', 'standards', 'search', 'parade', 'airwaves', 'you!', 'Mercer', 'write', 'lawyers', 'numbers?', 'construction', 'Make', 'redwood', 'possibly', 'Committee,', 'horses?', 'corner\"', 'rubbing', 'cell', 'illness.', 'behind', 'top!', 'Moosoleum.', 'That', 'tank...', 'carried', 'Yarn', 'business.', 'cart', 'underwater?', 'chaos.', 'sawdust.', 'down.', 'Attila', 'shop!', 'linuxmint', 'law?', '#They', \"'Jallikatu\", 'if', 'adopt', 'out*!', '-Ash', 'lot', 'midget', 'strength.', 'favorite', 'breaks', 'Put', 'different', 'dust', 'homie-hoe-stasis', 'rabbits', 'teeth!', 'mind', 'musical.', 'murder.', 'sun', 'Borscht', 'dry', 'Programmers', 'Fixed', 'circle?', '#throwback', 'find', 'showers', 'chasing', 'captivity.', 'Whatever', 'fan?', 'boogered', 'cars?', 'soccer', 'Philoppe.', 'subscribers', 'George', 'from.', '...charged', 'amused!...', 'terrain.', 'All', 'Plane.', 'Jose', 'an', 'Ghoster', 'battle?', 'zombie...', 'therefore', 'time?', 'ship', 'SSD', 'twitches?', 'medical', 'fast', 'waste', 'Raisin', 'hackneyed', 'butter?', 'Joe', 'GRAAAAINS', 'drove', 'elevators?', '**SLAP**!', 'flower?', 'obvious', 'chemists', 'P.', 'banker', 'Defunct', 'ironic', 'savannah', 'buffet', 'moves', 'drug', 'finish', 'silky', 'trouble.', \"Scrooge's\", 'cane**', 'strip?', 'around,', 'ahead', 'Knock-knock...', 'backed', 'she:', 'negative', 'Bagpiper,', 'holding', 'fancy', 'sheep', 'sailors', 'MOOstard.', 'taste.', 'disarmed', 'Pope', 'over\"', 'knife', 'scratching', 'picnic?', 'pages,', 'ever', 'hunter?', '9.', 'superhero', 'Belgium', 'conflict', '(True', 'help!', 'rack', '24', 'casket', 'alphabet', 'forgot', 'Grape?', 'orders', 'Wise', 'smoke?', 'faster,', 'marooned.', 'pairs', 'shooting', 'Noel.', 'roamin', 'filling.', 'newer', 'puss.', 'stool', 'kickstand?', 'Cholera', 'Post', 'online...', 'squishes', 'pasta?', 'Have', 'remove', '(x-post', 'cross', 'bear.*', 'raise', 'many', \"year's\", 'guess', 'cacti-lator!', 'long', 'neck', 'stalks', 'cents.', 'speaking.', 'must', 'tails?', 'tissues?', 'groups?', 'girls', 'Frozen.\"', 'chewed', 'pizza?', 'deer', 'explorer.', 'and....', 'everything.', 'years...', 'drowning?', 'color?\"', 'bend', 'there!', 'wife', 'CAPITALISM', 'staircase.', 'born', 'General', 'anymore?', 'class?', 'skipping', 'romaine-tic', 'Shall', 'enemies.', 'manure...', 'tsst', 'Harry', 'spilled', 'playing', 'key?', 'tells', 'crossed', 'On', 'obese', 'cheque', 'will', 'Samsung', 'calendar?', 'cake', 'singing', 'not.', '\"where\\'s', 'protons', 'memory?', 'french', \"I've\", 'door?', 'carats!', 'cowgirl', 'Chesterton**', '(Not', '42.', 'touching', 'jokes?\"', 'blew', 'shopping.', 'problem?', 'knight', 'appalled', 'Bail.', 'Today', 'dear', 'Czech', 'tree;', 'day...', 'madness.', 'joke', 'all!', 'claws', 'hebi.', 'moment', 'BAR!', 'tricks?', 'Fragile', 'twitch?', 'Elephants', 'Penguin!', 'day.', 'Reddit!', 'sailor...', 'might', 'King', 'Porcel', 'joined', 'joke...', 'hungry', 'waste.', 'maneuver.', 'pet.', 'Romanian', 'drugs', \"it'll\", '***OINKMENT!***', 'out', '(a)', 'hotel.', '\"It', 'shops?', 'colds', 'Robert', 'drink', 'mini.', 'flowers', 'immigraint.', 'healthy', 'Sinatra\"', '(this', 'Republic?', 'fluff.', 'joyful?', 'fedora', 'Round', 'iPod', 'roots', 'necro**mom**icon', 'one-eyed', 'graveyards?', 'Florida?', 'crappy', 'Clean', 'sleep?', 'Cattleacs', 'chandelier?', 'top,', 'Answer:', 'tuna!', 'Go', 'breath', 'attraction?', 'Entropy', 'fifth', 'glad', 'forth-grader', 'fourth', \"don't'.\", 'toenails', '(All', 'saying,', 'know', 'bathroom?', 'points', 'money.', \"'Cause\", 'Tachyon', 'spoke', 'no,', 'guy', 'and......', '\"C\".', 'herself', 'restroom?', 'Bert', 'Soup', 'leg?', 'world', 'ON', 'replied,', 'everything?\"', 'american', 'bear.', 'broker', 'guitar', 'amazed', 'account', 'bellhop', 'bi-polar', 'woof\"', 'instruments.', 'potato?', 'farmer', 'grade', 'ubiquitous', 'Mom', 'whack', '/u/KingOfRibbles', 'snowmen', 'joke-telling', 'Infection', 'system.', 'casting', 'me.', 'sewing', 'roof', 'candy', 'subreddit,', 'Sally', 'craziness', 'man.', 'Doorbell', 'gay.', 'sitting', 'eggs.', 'novels?', 'Bach\"', 'number', 'sake!', 'firemen?', 'Reddit..', 'interstate.', 'harpooned', '\"s\"', '...it', 'holds', 'bubbles.', 'trouble?', 'guitar...', 'two-farty', 'vendor?', 'mailman', 'disco', 'Nut', 'troublesome', \"rocket's\", 'Always', 'Pik-a-choo.', 'mummies?', \"Pete's\", '(No', 'understanding', 'cycle.', 'a', 'Penny', 'calling', 'start...', 'While', 'outside.', 'Donald', \"bacteria's\", 'snowball!', 'amoosing.', 'woke', 'lecturer?', 'turned', 'dryer', 'doctor', 'Beurre...', 'Ein', 'C#', 'snow?', 'jammed', 'pig?', 'asks,', 'let', 'point', 'espresso', 'Park', 'bird', 'Shepherd', 'discovered', 'synonym', 'Native', 'horse', 'recently?', 'Everywhere.', 'Toast-tah-dahs!', 'go]', 'duck', 'strongest', 'infeggtious', '-ahem-', 'Carl', 'weigh?', 'Energizer', 'TEAs', 'do.', 'child', 'IT!', 'swim', '\"To', 'Dam.', 'once...but', 'odd', 'stomp', 'later', 'cows?', 'quacktor', 'won?', 'Pls', 'numbered', 'Dear', 'times...', 'investigator', 'puppy', 'geologist', 'change...', 'US', 'comedy.', 'here?\"', 'rolled', 'bambooboo.', 'frisbee', 'climbs', 'tss', 'making', 'boy.', '*waist*.', 'learn', 'Cars', 'accident?', 'barber', 'green', 'beat', 'cars.', 'Mail', 'H.P.', 'from', 'Day?', 'pants?', 'de-lighted', 'detained', 'driving', 'conversation...', 'reception.', 'bovines?', 'break', 'around!', 'snowman', 'horror', 'finished', 'coffee:', 'stealing', 'crap', 'gallon', 'afford', 'positive.VCN', 'houses?', 'trying', 'La', 'baby,', 'works', 'even', '\"Who\\'s', 'threw', '\"That\\'s', 'what?', 'plane.', 'submitted', 'house', 'grandma', 'local', 'hostess', 'Congressmen', 'today', 'as', 'extrapolate', 'Santa', \"that'd\", 'serious.', 'French', 'supremacist', 'worse', 'tennis', 'moment.)', 'More', 'throat', 'you\"?', \"**Who's\", 'luggage', 'wombat?', 'gutter!*', 'pteranodon', 'Biscuit', 'Greg?', 'subreddit', ':', 'ohh', 'who,,,?', 'earthquake', 'discussed', 'corduroy', 'responding.', 'psychology...', 'cottage\"', 'say', 'tyrannical', 'Joke', 'University?', 'drinking', 'steals', \"'B.C.'\", 'mind,', 'Titled', 'Japan?', 'Pond.', 'prescribes', 'care.', 'alien', 'rebuke', 'payed', 'said', 'upholstery', 'religious', 'scrambled', 'want', 'grinder?', 'Beethoven', 'Paint?', 'nose?', 'Shout', 'school.', 'her', 'pessimist', 'moved.', 'hand', 'band!', '15', 'Guy', '1920', 'grilled', \"life'\", 'Watch', 'Tywalkasoreus', 'hamburglar', 'jokes', 'black', 'twice', 'come.', 'pearl?', 'Growth:', 'Italian.', 'thyme.', 'through', 'react', 'Houdini', 'slim', 'CHANGE?!', 'Scuba', '/r/Jokes)', 'try', 'pipes?', 'studied', 'Olive', 'told', 'purple', 'common', 'nothing.', 'pEGGy', 'heart', 'eight?', 'plate?', 'Bread', 'apricot', 'hear', 'ray', 'alkaline,', 'Sep-timber', 'scientists', '\"Oh,', 'angels', 'Amazon?', 'vampires.', 'giraffe', 'offices', 'chocolates', 'clocked', 'refuse', 'commercials?', 'buck', 'up!', 'build', 'turkey', 'Kim', '.', 'Chinese', 'C', 'jellyroll?', 'mare-a-thon.', '/', 'Any', 'Coco', 'today?', '\"fourth\"', 'bicycle?', '*Tank*', 'Swift', 'Muffet', 'distributor?', 'bloopers...I', 'yelling!', 'drums', 'Texan', 'iPad', 'arrive', 'order?\"', 'land', 'originally,', '\"who\".', \"Bulls'.\", 'town', 'arguing,', 'grammar...', 'schizophrenic', 'collection\"', 'defense', 'vegetable', 'To', '\"Rhino', \"Curie's\", '16', 'tender', 'Watts', 'man!', 'trace', 'stuff?', 'wedding?', 'have!', '~Skip', 'Straight', 'fairground', 'Attire...!!', 'podiatrists', 'kidnapped', \"treefrog's\", 'Malaria?', 'blood.', 'roof,', 'jazz?', 'alive?', 'Whos', 'parents', 'jump-roper?', 'letter', 'Hun', 'whiskey', \"moron's\", 'drop', 'twinkie', 'Yogoat!', 'very', 'psycho-path.', 'Irrelephant.', 'waist', \"YOU'RE\", 'coup', 'protein', 'honeycomb.', \"Combi's?\", 'seven?', 'wedge!', 'Tentacles.', 'say!', 'decided', 'Graaaaaaiiiins......', '~', 'prints', 'breaking', 'data', 'prune', '2:What', '!', 'tip', 'trees', 'master', 'WAY!', 'change!', 'color?', ':D', 'Bucket', '^^there?', 'episodes.', 'Quack!', 'William', 'Beatles', 'loves', 'Doo!', 'waits', 'sleevies.', 'better...', 'been', 'spud?', 'to?', 'bank', 'bookworm', 'swear', 'formation', 'his', 'Raidoactivity', 'start.', 'brings', 'stomach', 'picture?', 'cheese?', 'me?', 'Climbing', 'kids.', '*falcony!*', 'birth', 'win?', 'away', 'constipated,', 'hit?', '1)', 'sappy', 'twist?\"', 'perch.', 'yours?', 'gains', 'ever!', 'fear', 'milking', 'shop?', 'After', 'screw', 'boost', 'vegitaryan', 'operator.', \"'We\", 'purchase?', 'mistletoe', 'http://www.thedailyenglishshow.com/friday-joke/98-how-to-die/', 'Vine', 'part', 'Then,', '^mmmmbfmbm', 'can', 'return', 'who', 'boy', 'typically', 'paper!\"', 'say?', 'Personality.', 'creating', 'poodle-', 'asking', 'nearly', \"''I'm\", 'irony!', 'language.', '*How', '\"Knock-knock\"', 'neck-', 'abcdef', 'megahertz!', 'Laptop', 'cops', 'better', 'ropes', 'charities?', 'line,', 'potato', \"teacher's\", 'Whats', 'pro', 'groceries,', 'tail?', 'strong!\"', 'Loco', 'sunglasses', 'stole', 'possible', 'nosey', 'battery.', 'Estonian', 'dentist?', 'right', 'careful', \"frog's\", 'leg', 'thread', 'bother,', \"baker's\", 'Nevermind,', 'belt..', 'Rome', 'shredded.', 'Define', '(x-town)', 'bills', 'plug.', 'ground', 'attention.', 'hits', 'larger...', \"'n\", 'cars', 'people...', 'responded,', 'salad!', 'teeth.', 'paddy', 'whiskey?', 'Justice', 'night...', 'pen!!', 'cool!', 'step', 'worms', 'Planet', 'Cairo-practor.', 'vegetarian?', 'tonight.', 'marry', '\"Shhh!\"', 'Audi', 'Heard', 'totally', 'these', 'eyes', 'spoon', 'tame', 'for', 'skips', 'carrying', 'Arkansas.', 'intentions?', 'themselves!', 'one-of-a-kind', 'Pen', 'jackets', 'lifted', 'sly', 'once?', \"I'm\", 'Poppies', '\"fire\"', 'bite.', 'May.', 'Oh', 'side.', 'Flush.', 'celery?', 'yesterday.', '\"Quack,', 'restaurant', 'Howls', '[X', 'pee.', 'wears', 'appliance', 'Judge?', 'Dud.', 'first.', 'everyone', 'thread?', 'whey.', 'dictionary?', 'Gogh', 'flounder', 'cessation', 'outstanding', 'Pick', 'goer!', 'Bach.\"', 'Big', 'cancelled', 'bags.', 'Peas.', 'Super', '-\"Are', 'you.\"', 'bears?', 'donut!', 't-shirt', 'rockstar', 'Can', 'split', 'Student', 'backwards.', 'Too', 'Engagement,', 'owning', 'doors?', 'issue.\"', 'wondered', 'cannot', 'discriminate', 'Asian?', 'car?', 'hairy', 'Anubis.', 'reaction...', 'cold!', 'Swinestein.', 'socks?', 'normally', 'plastic', 'joke?\"', 'farm?', 'Kami.', 'Enjoy!', '[Denim,', 'condition.', 'cicadan', 'cookie..', \"What'\", 'saw', 'Broadway', 'byte', 'designed', 'field!', 'timeout?', 'Neck-tarine', 'Vel-crows', 'likes', 'Brown-chichen-Brown-cow', 'Trump', 'pickle.', 'hold', '~tips', 'depressed', 'Washington', 'ally.', 'hearts', 'Heh', '(not', 'richest', 'belt', 'my', 'spicy', 'decide', 'pun', 'mall', 'drink.', 'monk', 'Coming', 'rocks', 'her!\"', 'nordic', 'stale,', 'move?', '#foreveralone', 'racquetball', '*far', 'me', 'blurry.', 'door.', 'psychology?', 'taint', 'trunks', 'pooing', '-knick', 'scales!', 'eventually.', 'Decaffeinated', 'photographic', 'sports?', 'room', 'finally,', 'mass', 'laughing', 'right...', 'sleeping', 'spaceman?', 'Not', 'talking', 'Robin?', 'dyslexia', 'messed', 'proud.', '...is', 'Mlaria', 'consulted', 'napping.', 'lay', 'younger', 'Ernie', 'buh', 'God', 'wanted', 'Saccharide', 'ghosts', 'goo', 'ducks.', 'references', 'herbs?', 'explains', 'did.', 'Baghdad.', 'Nitric', 'wolves', 'attend', 'no-eye', 'day?', 'Apparently', 'Radio-Activity', 'outside...', 'Provolone', 'caters', 'Jokes', 'Ba', 'Man', 'see?', 'lot.', 'should', 'Oxide', 'whale', 'trees.', '^imagine', 'fresh', '3.14', 'zippo?', 'Omg', 'waved.', 'efficient', 'mathematician', 'gun', 'massive', 'clock', 'beers', 'speech', 'sing,', 'chamber', 'cause', 'pooping', '$0.99', '\"A', 'relationship', 'brick', 'oh-my-goodness,', 'desiccated\"', 'Chemistry', 'block', \"it'd\", '...are', 'probably', 'Likes', 'fruit', 'Caesars', '7', 'asked', 'Italian', 'Overheard:', 'asked:are', 'calendar...', 'faster.', 'pay', 'it...\"', '\"Leave', 'charging', 'Faux', 'complex', 'snack', 'invented', \"You'd\", 'brother...', 'amazing', 'thing.', 'Step', 'Russian', 'soft?', 'chromosomes.', '\"Where', 'dogs', 'breakfast?', 'egg', 'Why', 'Carey', 'Paula', 'gangster', 'Dustin', 'but...', 'amusement', 'elephant?', 'people!?', 'Truuuuuuuuuuu-moooooooooooooooooo!!!', 'turkey?', 'early', 'believe', 'bucks?', 'basket', 'needed', 'Age.', 'chickens', 'hygienist', 'boomerang.', 'One', 'great!', 'mirrors...', 'other.', 'ball?', 'seasoned', 'She', 'Libyans', 'wrist!', 'file', 'worried', 'did', 'Pardon', 'farmers', '\"-but', 'edda-cated.', 'younger.', 'story)', 'Myrrhder', 'Armadillo', 'three', 'Did', 'But,', 'bra?', 'Invisigoth.', 'away!', 'National', 'parking', 'figured', 'blonde', 'Smells', 'spice', 'loss', 'Buddhist', 'innovative', 'rock?', 'asks', 'veggieland?', \"can't.\", 'kin!', 'four-in', 'crew.', '9)', '1950s?', 'forward', 'flag?', 'men', 'tomato,', 'Jalapeno', 'pasture', 'interviewer', 'Santa,', 'Chicago?', 'machine?', 'dispute...', 'sausage?', 'staring', 'together!', 'burnt', 'whistle', 'maker', 'wondering', 'ambidextrous', 'separated?', 'plant.\"', 'prom?', 'cal', 'penguin!', 'Mathematics', 'set', 'Voldemort', 'seeing-eye', \"ABC's?\", 'hokey', 'model', 'Night...', 'working', 'India', 'Vine.', 'omg/omg', 'hated', 'clause.', 'Everything.', 'pierced?', 'store.', 'Thor', 'turbans', 'rudder', 'smoking', '^^Whose', 'Prize?', 'found', 'addicted', 'it!', 'Gerber', 'really', 'two.', 'looking', 'dino', 'lab', '\"Have', 'Pink', 'invoices', 'organ', 'puffin!', 'dish!', '\"Smiles\"', 'glue?', 'Luftwaffles', 'ridiculously', 'muttered.', 'looked', 'corner', 'dragoon?', 'journey', 'Michelle\"', 'proved', 'miles.', '(As', 'mooooo-vies.', 'pirates', 'hand?', 'Honey.', '..A', '\"behind\"', 'it..', 'talk', 'Ton', 'oldie', 'help', 'me;', 'rabbit?', 'magnitude?', 'raping', '(Still', 'boat', 'website.', 'shucks!', '\"no,', 'Find', 'RAINBOW****', 'capital', 'P', 'back', 'jooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooke', 'Old', 'also', 'Shang-bye.', 'Jersey!', 'harm', 'pilot.', 'nemesis?', 'skeleTon', 'Cherry-atric', 'drive!', 'supporting', 'coupe', 'wonderful,', 'Missouri.', 'eat', 'arrrrrrrrr!', 'Today,', 'planets?', 'Y', 'street...', 'Found', 'at', 'cafe', 'with?', 'Coffee', 'beneath', 'Living', 'Fartender', 'Bran', 'fly?', 'Rooster:', 'talked', 'cow?', 'pepper', 'chill', 'Or', 'stone', 'mind.', '*From', 'candy?', '14', 'martini.', 'Nate,', 'shoes', 'angle', 'dad', 'sleep.', 'named', 'frankly,', 'cantaloupe!', 'Eileen', 'shouting', 'musician?', 'broke\"', 'bigamist', 'jerky', 'Fourba!', 'satisfy', 'date', 'stable', 'kraft', 'Anyone', 'landed', 'rabbit.', 'horse\"', 'one', 'Street!', 'Juliet', 'CPU', 'Year', 'hoarse', 'man,', 'hangin\\'?\"***', 'tyrannosaurus', 'State.', 'envelope...', 'even*!', 'hat,', 'peng-lose.', 'quiz', 'deniability.', 'bloated', 'husband', 'truthful', 'Siri', 'duckerate', 'butter*', 'lounge', 'Says', 'nor', 'confused?', 'stab', 'suits!', 'double-amputee?', '\"Kelp', \"They're\", 'Woodrow', 'Get', '\"Okay.', 'land?', 'sick', 'Laffy', 'actually', 'mate.\"', 'rows', 'strange.', '\"Nah,', '*', '#', 'pillow?', 'divorce,', 'wrong.', 'dinar.', 'deal', 'South', 'road?', 'returns,', 'budapest!\"', 'beer', 'another', 'item?', 'which', 'human', 'bored!', 'welcome*', 'physics', 'peacefully', 'bath?', 'bicycle', 'WE', 'street', 'oinkment...', 'college', 'Decomposing', 'Patriot', 'Had', 'muffler', 'boiling', 'Great', '5,6,7,8', 'Mozart', 'protagonist?', 'heavy,', 'Hitler', 'Roverdose](http://i.imgur.com/BtyF5ys.jpg)', 'everywhere', 'paws-able', 'weak.', 'eastern', 'about', 'Greyed', 'sand', 'Neigh-boars.', 'Their', 'Do', 'caterpillars', '**G.', 'crippled', 'aboard.', 'tree?', '...so', 'like...', 'Pride', 'Starcraft:', 'abcdefghijklmopqrstuvwxy', 'toy', 'office', 'embarrassed?', 'fired!', 'later,', 'squat(s).', 'Carny-vore', 'light', 'donut', 'weigh', \"Why'd\", '50', 'working*.', 'double', 'dwarves', 'Macho', \"roommate's\", 'awful', 'Roosters', 'corner!', 'files', 'Dentist?', 'Quija-13', 'Does', 'away.', 'computer', 'dominate', 'SMILES', 'Communist', 'ya', 'call...', 'ion', 'understood?', 'something', 'Thesaurus', 'anti-gravity...', 'that?\"', 'components', 'closed...', 'Augustus', 'cooking', 'organize', 'stein.', 'Pikmin', 'released', 'Say', 'Spelling', 'complex.', 'missing?', 'watering', 'hours', 'Pirate', 'fires.', '...with', 'Dijon.', 'more?', '\"You\\'re', 'arctic', 'fences', 'move', 'car', 'Nobody', 'farming?', 'vegetarian', 'given', 'week', 'neeiiigghh?', \"Time's\", 'you..', 'sanitizer!!!', 'People', 'shot', '8', 'people;', 'knocked', 'knows', 'jumper', 'headlines.', 'cream?', 'mother', \"One's\", 'minor.', 'instrument?', 'robbers?', 'c:', 'poorly', 'poultrygeist!', 'tomato.', 'scents...', 'trouble', '...\"', 'cartridge?', 'however:', '10,000', 'dirty.', 'go?', 'just', 'machine.', '*whipped*!', 'You!', 'bees', 'play.\"', 'tail.', 'music?', 'Time', 'flying?', 'clerk?', 'win,', 'miss', \"cloud's\", 'random', 'Pac-Man', 'business', 'bears', 'drew', 'girl.', 'lamb.', 'behave', 'yellow.', 'beef?', 'porridge', 'shark', 'circles.', 'chains', 'T-rex?', 'genre', 'attack?', 'Gandhi', 'novel', 'perpetually', 'giddy', 'Penny\"', 'shortages', 'highest', 'guts!', 'stereotypes', '\"Look', '2:30', 'abroad.', 'retired', '..', 'draw', 'vowel', 'live?', 'husky', 'pair', 'corner.', 'Pi', 'or', 'colorblind', 'grow.', 'built', 'flaming', 'burn', 'clock?', 'Rod', 'dancers', 'frostbite!', 'keyboard', 'forest?', 'jokes.', 'Vulgar', 'going', 'pants', 'laptop...', 'Ow!', 'Mobius', 'Platypus', 'party?', 'snowman?', 'drinks', '...if', 'stare&gt;', 'date!', 'your', 'one.', 'up!!', 'mud.', \"grain's\", 'awareness.', 'them.', 'dinner', 'packaged', 'Tennessee?', 'ferrous', 'tree', 'surprise', 'there', 'always', 'For', '\"Flying', 'it!!!', 'shopping', \"'do...\", 'dedicated', 'fog.', 'stand.', 'amusing', 'nevermind.', 'Strips.', \"He'd\", 'Just', \"Here's\", 'bunch', '*Skip', 'thumb', 'Megabytes...we', 'horses)', '\"Help!', 'wheels', 'TIRED!!!', 'reluctant', 'Look', '\"don\\'t', 'promoted?', 'attention', 'Stark', 'choose', 'dollars', '*Decomposing*', 'gig', 'fluke.\"', 'food', 'phrase,', 'accidents', 'programmers', 'colour', '*tips', 'city?', 'z', 'covered', 'fake.', 'far', '**Jane:**', 'drag.\"', 'Them:', 'farted', 'impediments?', 'God.\"', 'week...', 'calendar', 'Deadpan.', 'nut?', 'antenna?', 'somewhere', 'http://redditmetrics.com/r/cleanjokes', 'Protoss', 'Britain?', 'Freak.', 'spit', 'mommy', 'sinkholes?', 'Loki.', 'environment.', 'Investigator.', 'word', 'completely...', '\"snoop\"?', 'towns', 'incontinent.', 'classical', 'tomato', 'muscle?', 'recommend', 'Finnish', 'players', 'operation?', 'steer', 'lousy', 'circles', 'Note', 'summer', 'dressing.', 'B', 'by', 'beet.', 'onion', 'carefully', '[A', 'Hey,', 'bar', 'Dating', 'Koldcutta', 'brothers', 'proteins', 'sweetest', 'razor', 'themselves', 'taser...', 'it)', 'Shame', 'balanced', 'Unlike', 'Frozen', 'mallard', 'puns.', 'Jamaican', 'install', 'Christian', 'but', 'radiation', 'Guardians', 'absolutely', 'happened', 'Friends', 'Martians', 'bakery', 'Irony', 'elevator,', 'panda', 'curry', 'numeral.', \"Shatner's\", 'Neither', 'Trombones!', '--From', 'Vegas?', 'pie?', 'Sundae', 'Q)', 'university.', 'post:', 'Acha.', 'denim!', \"Arthur's\", 'accountants?', 'again!', 'jooooke!', 'oh', 'rhymes', 'Still,', 'like-a', 'Jackie', 'joke:', 'scientist', 'transportation?', 'hoop', 'Pears', 'cant', 'What', 'writing', 'jeans...', 'he:', 'end', 'Michael', 'Wisconsin?', 'repossessed!', 'Psycho-paths.', 'thrusting', 'dressing', 'five', 'room?', 'sorority', 'puts', 'stands', 'Bach.', 'Everyone', 'calluses', 'Boogie', 'Middle', 'dwarf?', 'business?', 'Fine.', 'social', 'wi-fi', 'Foil', 'golfers', 'Barely', 'against', \"Beethoven's\", 'servant', 'math', 'outta', 'gym?', 'scripture:', 'couple', 'Dustpan.', 'frog', 'welcome', 'comes', 'thing?', 'peels', 'immediate', 'anywhere.', 'remember', 'rid', 'remembered', 'Punchbug?', \"you're\", 'thinking', 'suspect', 'word!', 'Constipated?', 'MAYBE!\"', 'laziest?', 'tube', 'doing.', 'Bad.', 'thoughts', 'Anakin', 'done.', 'green,', 'proper', 'Scholar', 'says', 'where', 'due', 'nizzle.', 'Breathe!\"', 'fell', 'storytellers?', 'court?', 'PETRI', 'lobster', 'idiot!', 'helpler', 'barista', 'its', 'rate.', '(This', 'tooth', '*(looks', 'Japan,', 'eek\"', 'medal...', 'pear?', '**decaf**alon', 'contest...', 'refused', 'Fall', 'pierce', 'SPACE', \"What's\", 'paint?', 'dodge', 'interest', 'Goku', 'Poptometrist!', 'one!', 'flu', 'News)', 'street?', \"*You're\", 'centipede', 'feel', 'twenty', 'pumpkin?', 'goeth', 'peas', 'OUTSIDE!', 'man?', '...theres', 'accessible', 'fully', 'table?', 'think', 'fly,', 'offspring', 'monsters', 'link', 'eye-deer.', 'up?', 'Sin/Cos', 'least', 'save', 'boulder!', 'criticize', '\"Well', 'down', 'peas!', 'skydiving?', 'box', 'Marion.', 'to...', 'hoes', 'welcomes', 'mars', \"child's\", 'concert', 'introduce', 'knotty.', 'shoes?', 'Decaf', '\"Oh', 'sock?', 'discounted', 'attack', 'us', 'factory', 'alcohol', 'rice', 'leopard', 'thought,', 'ears?', 'sock)*', 'career', 'husky?', 'drinks...', '1.', 'somewhere.', 'hamburger?', 'Doyouthinkhesarus', 'goes', 'fault', '\"I\\'ve', 'Bless', 'hafta', 'Gorgonzola.', 'words', 'slice', 'bondage', 'Heifer-weizen.', 'haircut.', 'mirage,', 'Right', \"carrots?'\", 'Connery', 'cold', 'adjust', 'yesterday!', 'Plump', 'called?', 'since', 'ugly', 'egg?', 'resolution?', '*fowl*', 'Diana?', 'sneeze!', 'friend?', 'unlocking', 'except', 'wipes', 'Catholics', 'pool.', 'Tarbucks.', 'pirate', 'interest.', 'quality', 'production', 'Q:', 'swimming', 'Ergo', 'Our', 'Carbon', 'thesaurus', 'bar...', 'immediately', 'shoestring', 'trips', 'Tyler', 'job!', 'having', 'came', 'I-scream!', 'carnivore.', 'UNCANNY!', 'Red,', 'go!', 'chin', 'garage', '-and', 'leaving', 'yourself', 'fired', 'co-', 'consider', 'people', 'sure?\"', 'Dinners', 'cargo', 'Norse', 'moose?', 'Max', 'jalapeno', 'world?', 'THE', 'delivery', 'cons', 'Tin', 'co-worker', 'him', 'Igloos', 'Batman', 'runny', 'billed', 'cost', 'Stairs', 'anyway.', 'wave', 'scoop!', 'dad,', 'Ash', 'quack!', 'state?', 'foot', 'dum', 'most.', '\"breakfast', 'reagent.', 'banana', 'acids.', 'czech-e-cheese', 'platypus', 'cereal', \"what's\", 'Drew', 'street,', 'sheared', 'BA-NA-NA-NA!', 'Saturday?', 'knock!', 'brew?', 'replacing', 'home.', 'appetizers', 'buy', 'made', 'gorilla', 'snatchers?', 'rights.', 'wind', 'story', 'broke', 'train', 'seizure', 'locomotive', 'poultry', 'dig,', 'pulled', 'kitchen', 'nine', 'wallet.', 'dressed', 'bucket.', 'Launch', 'Montana', 'take?', 'School', '**ton**', 'trap.', 'liquid?', 'kangaroo', 'body-builder', 'topic', 'Hoffman?', 'firstborn', 'brass', 'eyes?', 'pengrin!', 'idiot', 'exams', 'performance', 'WWII', 'May', 'octopus?', 'today....', 'A.', 'called', 'performed', 'toilet', 'not,', 'Mystic', 'towards', 'astronomy', 'ones?', 'years.', 'read', 'Police', 'Between', 'drive?', 'Prints.', 'sleevies!', 'white', 'greatest', 'on?', 'cow', 'condition.\"', 'alphabet.', 'tickles', 'buns.', 'Thanks', 'Im', 'hipsters', 'dont', 'magician', '6:30', 'In', 'Google', 'pie', 'lift', 'banned', 'scam', '\"Breathe', 'regrets', \"You're\", 'X-post', 'librarians', \"Skywalker's\", 'unlock', 'grocery', 'age...', 'Pac-Man?', 'alter-eggos', 'report?', 'Cairopractor!', 'dude', '\"Bach,', 'tonic\"', 'December', 'helping', 'swings?', 'feed', 'past', 'organizing', 'kittens?', 'Dr.', 'baby', 'Shell', 'Mommy!', 'grayvy.', 'karate?', 'unique', 'repairman', 'castanet', 'turtle', 'Henry', 'Age', 'yellow', '*wave', 'sound', 'Corsica?', \"'ell\", 'cheesy.', 'naked?', 'juice', 'this', \"chicken's\", 'in?', 'do,', 'full', 'naming', 'deer?', 'alphabet...', 'Franklin.', '[A.K.A.', 'joke/first', '\"July\"', 'soup?', 'aquarium', 'bound', 'marks', 'coworkers)', 'fake?\"', 'pleased', 'forget-me-knots.', '*unique*', 'grandfather', 'post', 'suspense?', 'soap', 'halloween?', 'Thor.', 'Rex', 'WW2?', 'amusement:', '...all', 'bone?', 'penguins', 'Rebecca', 'tricycle', 'invisible', 'memory.', '\"Nice', 'weave', 'sentence?', 'bacteria', \"'th'\", 'boos.', 'plum?', 'now!!', \"she'll\", 'Mexican', 'browsing', 'Scanthesku', 'carrots.', 'Marley', 'similar', \"Kellog's\", 'melons,', 'Lukewarm', 'disease', 'fed', 'places.\"', '\"You', 'K.G.B.', 'kinds', 'ancient', 'Watson,', 'Bale.', ':)', 'race?', 'post-humorously!', \"-Who's\", 'licked', 'Nobel', 'hand.', 'illustration.', 'belt!', 'scarecrow', 'Tongs.', 'laugh', 'negi', 'speak', 'cant-aloupe', 'face?\"', 'clubs?', 'kids...', 'chamomile?', 'television', 'share', '\"Will\"', 'Disney', 'Sadly,', '7up', 'ball-point', 'birth?', 'sad', '~~Mop', 'rex', 'denims?', 'school', 'hang', 'Crispin', 'typist', 'bus?', 'wallpaper', 'result.', 'Want', 'munchkins?', 'latest', 'any', 'neutron', 'primary', 'retailers', \"He'll\", 'beef...', 'No', 'pop', 'there?\"', 'chokes', 'punchline,', 'killer.', 'racist.', 'isss!', 'Vin', 'stay', 'tail', 'cheese', 'Nantucket...', 'step!', 'questions!!', 'snapshot.', 'contributions?', 'shrine', 'bros', 'layoffs', 'catch', 'gluten', 'depressed?', 'Knight', 'empty', '\"Don\\'t', 'trust', 'brewmaster?', 'waaaay', '...for', 'asked?', 'dinosaur?', 'obstacle', 'reverse', 'ardent', \"world's\", 'color', 'burglar', 'degree', 'to\"', 'giving', 'bands.', 'IM', 'response', 'cooler?', 'Silento', \"aren't\", 'swamp?', 'letters...', 'resistance?', 'hipster?', 'judge', 'pact.', 'definitely', 'everyone.', 'sounds', \"Diana's\", '/r/cleanjokes!', 'credit?', 'last', 'some', 'door', '\"Chew,', 'Romaine', '10K', 'bring?', 'old,', 'years', 'mean', 'rap', 'r/Fantasy]', '(Revise', 'shutdown', 'grave', 'Whale', 'plusses.', 'vacuum', 'college?', 'connects', 'day,', 'audio', 'turnip', 'Kirk', 'Vulcan', 'section', 'battle', 'together...', \"haven't\", 'antelope', 'gym', 'Pilgrims', 'state', 'mildew-', 'together', 'tomatoes', 'dumb.\"', 'pancakes?', 'romance', 'humor', 'Fresh', '...', 'planet', '(I', 'nonsense.', 'bright.', 'ink?', 'planet?', 'nails', 'Heart', 'Solo.', 'loud', 'apples', 'poached', 'Zuckerberg?', 'bath!!!', 'space?', 'card!***', 'Question', 'idea)', 'illegal?', 'Lost.', 'Bonaparte', 'blush?', 'entrees,', 'loudest', '\"I\\'m', '\"stable', '\"most', \"*can't\", 'armed', 'request', 'stationery.', 'europe?', 'tired.', 'frog?', 'socks,', 'jack', 'James', 'roaring?', 'Man)', 'unit', 'none', 'Affleckted', 'broad', 'tossed', 'cute', '(Puns', 'actor', 'Americans', 'nosy?', 'dental', 'out,', 'gives', 'tripped', 'strongman', 'arcade', 'unemployed...', 'vest?', 'posting', 'elk?', 'chocolate', 'baa', 'Freak', 'Bison', 'grape?', 'have?', 'lot,', 'mix', 'balance.', 'laughtose', 'sandwiches?', 'phrase', 'V', 'with', 'buccaneer', 'Farmer', 'wife?', 'bull?*', '/r/askreddit', 'Reisens...', 'were', '****', 'cashier', 'tomorrow.', 'self', 'walk!', 'Bigamist', 'elephants', '*Caesars*', 'soup.', 'unforeseen', 'Pumpkin', 'Premium', 'uniforms?', 'eaten.', 'four', 'it.', 'feat.', 'jungle?', 'Dumpty', 'stallion', 'plus', 'sentence', 'side!', 'presented', 'clothes..', 'tractor?', 'elephant', 'blue', 'buccaneer.', 'Best', '[My', 'played', 'japanese', 'Hedger](https://www.facebook.com/HedgerHumor/photos/pb.630201143662377.-2207520000.1443863939./1179935295355623/?type=3&amp;theater).', 'white,', '\"Huh,', 'another:', 'Terrormisu', 'bulbs?', 'top?', 'Iron', 'Last', 'Password', 'Spy', 'pot', 'cheddar', 'complemented?', 'Bee', 'hybrids!', 'fold.', 'binary', 'Curt', 'belt.', 'teeth', 'funk.', 'sour', 'wantonly', 'scripture', 'shell', 'games.', 'wire', 'imaginary', 'desk', 'Left\",', 'funnier', 'Fan.', 'over', 'felt', 'takes', 'fssshhh...', 'forest', 'common-tater!', 'six', 'Better', 'Quite', 'sodium', '*THUD*', 'MetLife', 'photon', 'hives.', 'grandpa,', 'fish?\"', 'old', 'tickles.', 'wa-waa-waaaa!', 'removed!', 'ewe', 'Masahiro', 'like', 'Camembert!', 'decision.', 'Free.', 'impossible', \"'Na,\", 'Poor', 'http://www.dadlaughs.com', 'moos.', 'ads', 'squirrel', 'turnover', 'Dre', 'Like', 'Duck', '[Works', 'trip', 'while,', 'pteredacted.', 'Cork', 'potatographic', 'cup', 'water?', 'instead', 'seasons?', 'town?', 'round?', 'rotten', 'engineering?', 'clown?', 'leather,', 'HELP!', 'sitting...', 'spaghetto.', 'sick?', 'Die?', 'denim', 'dolphin', '(my', 'middle', 'bride?', 'outside', 'breath,', 'fashion', 'atmosphere', 'lighter.', 'stunning!', 'meat', 'miles', 'country', 'Hefty', 'Sherlock...\"', 'followed', '(state', 'grill?', 'realization...', 'board)', 'Dogg', 'tag.', \"Gordan's\", 'sat-nav.', 'CHEETAHS!', 'anything', 'capitalize', 'starts', 'pokey', 'n?', 'forced.', 'whoever', 'sturgeon.', 'gold', 'weird.', 'plain', 'suppository.', 'feet', 'Elvis', 'Elephino', 'By', '99', 'Glass', 'glass,', 'Definitions', '...and', 'Maybe', 'iPods', '\"COUS,', 'CLEAN', 'LOTR', '100%...', 'Request:', 'Mother', 'newest', '\"Welcome', 'out)', '\"darn', 'Poke', 'are:', 'sister', 'house.', 'English', 'stamp', 'ledge', 'Bangladesh?', 'area', 'I4', '-that', 'swamp-dwellers', 'Brown', 'Magnus!', 'house?', 'kill', 'succinct', 'Cosmos.', 'eye?', 'burning', 'ballpark', 'clucky.', 'denim.](http://www.youtube.com/watch?v=c0SuIMUoShI)', 'bad', 'develop', 'Ramsays', '2016', 'Dickens', 'Fryday.', 'real', 'Physics?', 'ducks', 'Scary', 'Albacoreque.', 'mumble-bee.', 'Abby', 'personal', 'cicadas', 'Kane', 'Alaska?', 'pickles', '\"Hahahahaha...*thud*\"?', 'Bake', 'Nightmares', 'we', 'glasses,', '\"S\"', '(Roberto)', 'eight.', 'backward?', 'Charles', 'ours\".', 'months', 'ago,', 'achoo!\"', 'them\".', 'experiment;', 'arent', 'seems', 'funny', 'agitated', 'much!', 'space', 'digital', 'avoid', 'smooth', 'days.', 'steamroll', 'Darth', 'type?', 'rule', 'cornylamejokes', 'belt.\"', 'tears', 'jungle', 'bar?', 'potatoes', 'ditch.', 'site', 'know!', 'mummy', 'From:', 'Rabbyte!', 'church,', 'format', 'sedan!', '\"make', 'rules', 'Dublin.', 'Fraud', 'proverb:', 'pass', 'their', 'Descartes', \"isn't\", 'departure', 'there:', 'Are', 'vacation?', 'scale', 'light.', 'women', \"Balloon's\", 'outloud', 'No,', 'well.', 'change', 'duck..', 'fodder', 'Bel', 'School.', 'wants', 'inked?', 'chemists?', \"i'm\", \"There's\", 'think,', 'Rabbit,', 'webbed', 'sweets', 'wish', 'bruises', 'mis-steak.', '1/8', 'expecting', 'Lama', 'marathon', 'language?', 'themed', 'vinegar', 'gobble', 'P.S.', 'Galaxy', 'poet,', 'food!', 'spill.', 'seat...', 'Sahara?', 'is', 'Orbit', 'hotdogs?', 'join', 'side?', 'coincidence...', 'allowed', 'windshield?', \"Chan's\", 'Lays.', 'Savage', 'curry?', 'cheapest', '*knee', 'Kids,', '[Thanks,', 'cold?', 'Offspring', 'Seafood', 'together.', 'Klaustrophobic!', 'Moovies', 'saloon', 'so...', '***P***', 'washing', 'fish', 'means', 'describes', 'dirty-\"', 'woman?', 'Randy', 'first,', 'Nomads!', 'ice-cream?', 'Skill', 'acronym:', 'dentists', '&lt;p&gt;', 'C.', 'periodically.', 'boats', '(sounds', 'scales', '30th', 'mad?\"', 'device', 'swine', 'butcher', 'Milk', 'SC', 'unless', 'EXcavator', 'Oh,', 'front', 'world.', 'are', 'sing', 'lightbulb?', 'woman.', 'Finally', 'two?', '\"Meet', 'redditor', 'isnt', 'somebody', 'divorce', '!!!', 'children', 'steps.', ':D)', 'done', 'dishes', 'presents...', 'for?', 'porcupines', \"It'll\", 'security?', 'show', 'Because', 'alligator', 'slap*', 'it?', 'crows?', 'majors', 'him,', 'anything.', \"didn't\", 'football', 'hacker', 'Square', 'surrounded', 'ears', '\"plagiarism\".', 'crew', 'shellfish.', 'rainbow', 'beak', 'Russia', 'Deja', 'service?', 'stick', 'volleyball?', 'suit.', 'with.', 'dead', '(as', 'Similar', 'Curses!', 'belly', \"fly's\", 'food,', 'seaweed...', 'U2?', 'Who', 'announcing', 'world...', 'Asks', 'poop', 'coming.', 'gummy', 'WAS', 'of?', 'are...', 'hide', 'key.', 'brown', 'Auuuuuuuuuuuuudis!', 'dig', 'window', 'Tennis', 'swimming?', 'grounds', 'traditional', 'wrapped', 'red', 'night-', 'weights.', 'talk?', 'Wi-Fight', 'inflation?', 'Russian.', 'floor', 'cooks', 'case', 'waiting', 'PC,', 'Dayton', 'cellist', 'son?', 'European.', 'Cash', 'Wilson', 'close', 'Wimbledon?', 'jokes,', 'still?', 'Q.', 'porky', 'coffin?\"', 'idealist', 'Actually,', 'crying?', 'bouqet', 'up...', 'Boobies.', 'Africa?', 'firefly', 'halos...', '4/10', 'horns', 'levels.', 'grade.', \"don't.\", 'Total', 'around.', 'Wata', 'butched', 'How-Ling', 'grape', \"...that's\", 'bumblegee', 'flowers,', 'BAAAACH', 'socialism', 'Grape', 'cold,', 'tuba?', 'rooster', 'serial', \"Who's\", 'Talk', 'Little', 'online?', 'here!\"', 'Ouch', 'Colonel', \"'React'\", 'electron.\"', 'pro-tractor.', 'makeup', 'Finnish.', 'book', 'Redbox,', '3.', 'shaped', '\"So', 'factory...', 'March?', 'snoop?', 'routine?', 'prince...', 'password', 'mexican', '21st', 'they', 'safety?', 'peach?', 'Station!', 'fly', 'Francis', 'weight?**', 'Wagon', 'ordering', 'birds,', 'south', 'had', 'already', 'stare', 'ice.', 'day', 'table.', 'number....', 'Duke', 'same', '\"Elementary,', \"doctor's\", 'Dam', 'musician', 'sings', 'tornado.', 'A)', 'chassing', 'there?**', 'coal?', 'alcoholic...', 'flows', 'go.', 'travel', '\"', 'Ethics', 'lunch!', 'Race', 'Difference', 'terrible', 'Girlfriend', 'pause?\"', 'Something', 'bucket?!\"', 'chrome', 'dance', 'bladder', 'vampire', 'one-sided.', 'Stick', 'coke.\"', 'transgender...', 'cus', 'Paint', \"Patrick's\", 'times.', 'Romans', '*too', 'Source:', 'iron?', 'meowtain.', 'secretary', 'Chech', 'ago...', '\"diesel', 'hamburgers?', 'pink', 'Sycamore.', 'Germany', 'dumb.', 'team', 'disorder...', 'Garbage', 'FBI', 'Fez', 'bro', 'Ireland', 'soap.', 'girlfriend', 'nearest', '1:**', 'side', 'slab', 'priest', 'flat.', 'pantyhose', 'Eskimo', \"how's\", 'American?', 'Seaweed!', 'sure?', 'booby', 'tank,', 'urologist?', 'in.\"', 'water.', 'PBS', 'need', 'Mordor\"', 'mouth', 'doing', 'short.', 'marsupial?', 'Charades.', 'sad?', 'cream', '[PICKLE]', 'puddle.', 'nostrils?', 'rhythm', 'bucket...', \"he's\", 'Twitter', 'shade.', 'coat...', 'wrongs', 'anyone', 'whats', '/r/PeterL', 'Oli-Mart', 'rapper', 'coal', 'who?\\'\"', '***sombrero', 'across', 'burned?', 'head\"', 'steps', 'imagination.', 'Circle?', 'itself?', 'right.', 'book?', 'deep-sea', 'litter', 'Nose', 'chicken...*', \"they're\", 'dog.', 'A:', 'lions', 'bad.', 'and', 'skills', 'Kazakhstani', 'incomplete', 'eventually', 'food.', 'does', 'gonna', \"Don't\", 'alcohol?', 'angry?', 'stock', 'trainer.', 'toad-aly', 'fame', 'famous', 'freshener', 'slides', 'seconds.', 'Fall.', 'bulimic', 'industry.', 'Humphrey.', '**Congratulations,', 'worry,', 'Dung.', 'ointment...', 'asleep', '\"Do', 'gets', 'Fry\"', 'No-eye', 'married', 'dead.', 'jobs', 'Launch!', 'metal?', 'great.', '\"Dye', 'No.', 'Lost', 'rang', 'O', 'here!', 'Sinatra', 'center', 'thought', 'illusion.', 'piano', 'barium.', 'catholics', \"Sagan's\", 'now', 'now.', 'bodies.', 'collection?', 'shredded', 'maaan*.', 'steam-roller', 'dickens', 'Innuendo', 'spider?', 'fall?', 'kept', 'new', 'attached.', 'Steak.', 'Kangeroo,', 'fall', 'difficult', 'gramma', 'clothes-minded.', 'brick.', 'turns.', 'mood', 'Sorry.', 'code', 'Thanks!', 'literature', 'bent', 'skin', 'hitting', 'buddy', 'reads!', 'earlier.', 'Tim', 'cheesiest', 'coffee', 'back?', 'Carr**', 'Someday', 'take', 'church?', 'Joke]', \"'Get\", 'Jamboree', 'boring.', 'million', 'agnostic', 'became', 'hear?', 'convention', 'glasses?', '-so', 'sows', 'list.', 'included.', 'silk', 'allow', 'Mary', 'eat!', 'About', '*bee*ch', 'Advanced', 'air', 'CNN?', 'The', 'beautiful', 'Hollywood.', 'corn', 'ride', 'during', 'pony', 'serves', 'S!', 'scrub', 'stuck', 'Beautiful', 'herd', 'idea', 'blame', 'Wars', 'circus.', 'invest', 'underwear?', 'either.', 'Accidental', 'himself', 'sent', 'van', 'tooth,', 'animal', 'Wonton', 'freight!', 'Took', 'Attempted', '...they', 'sub', 'farts', 'statue?', 'itself.', 'vicious', 'much.', 'Dinosaurs', 'zero', 'orphans?', 'operation.', 'Napoleon', 'hippo', 'problems.', 'delighted.', 'Hot!', 'mildly', 'patch!', 'Callused', 'Soviet-era', 'done?', 'high)', 'cornflakes.', 'lts', '25', 'gorillas', '20?', 'in...', 'baker', \"tryin'\", 'Carrots?', 'aged', 'Impatient', 'Affogato.', 'falling', 'soda', 'hallucination?', 'preacher', 'Arcane-gel!', 'jeans?', 'actual', \"'Do\", 'dip', 'stories', 'Stewart', 'emergency!', 'living?', 'sell', 'snails?', 'do', 'Captain', 'out!', 'ice', 'Bean', 'day!', 'walk?', 'Nacho', 'tooth?', 'Swimming', 'Your', 'Dhaka', '\"How', \"shouldn't,\", 'night,', \"Barq's\", 'fork', \"voila'!\", 'whale,', 'Dog', 'carrots', 'addicted...', 'outstanding.', 'backs?', \"comedian's\", 'pencil.', 'FRAMED!', '-', 'common?', 'Pillsbury', 'Caesars.', 'beginning.', 'bones?', 'cleaner', 'annoying', 'kitchen,', 'dealer,', 'government', 'teacher', 'humour.', 'hot', 'pyramid', 'Sturgeon', 'efficient,', 'sea', 'business!', 'mine?', '*pepper*', \"hadn't\", 'died?', 'Now', 'Benedict', 'warranted.', 'artificial', 'antennas', 'boring', 'camper!]', 'fake', 'elbow', 'pointed', 'twins', 'interview', 'February', '0?', 'marine', 'Bruce', 'extreme', 'LeBron', 'Na.', 'BACH', 'zoo?', 'arm', 'cow.', '\"Divorce', 'final', 'drummer', 'Triangle', 'sorry,', 'slept', 'Hose', 'dream', 'economics', 'Kids', \"reporter's\", 'crazy', 'smells', 'Cheese', 'anymore.', 'retail', 'balance,', 'boogie', 'Romeo', 'running?', 'Brain', \"don't,\", 'around', 'gathering', 'Interrup........', 'mythical', 'wolf?', 'quit?', '**/r/cleanjokes', '...walks', 'prepare', 'hearing!', 'tea', \"goin'?\", \"tuna's\", 'Q)What', 'Breaking', 'diet.', 'leek', 'fight?', 'God.', '\"incorrect\"', \"here's\", 'IIRC.', 'Control', 'temperature', 'bit', 'bartender', 'F1', 'babies.', 'NASA', 'internet?', 'creepy', \"'What's\", 'realizations', 'batman', 'answer', 'Those', 'casinos', 'keen', '...but', 'civilized', 'brain', 'videos', '=', 'cheetahs', 'Paws', 'meteor', 'fear?', 'Let', 'Pay', 'economist', 'transit', 'Drizzle', '(Also:', 'address', 'wooden', 'salt', 'half', 'woman...', 'down?', 'I`ll', 'died', 'stairs?', 'hiding', 'insist', 'enjoy', 'clothes?', 'grain', 'meal?', 'scares', 'counts.', 'hopping', 'down!', 'Hot', '?', 'cheep,', 'chef', 'Tootsie.', 'days', 'emails', 'edition.', 'poison', 'pack', 'dismay,', 'c/o', 'shoes.', 'Jong', 'bookworms', 'be', 'guys', 'mobster', 'hill', 'donor.', 'count', 'from?', '/r/pickle', 'band', 'fell.', 'chicken!', 'head.', 'Autumn', 'wildly', 'bike...', 'mile', '60...', 'Alaskan', \"pirate's\", 'culture', '....a', 'apologize?', \"Hamburglar's\", 'question', 'With', 'Karl', 'everything', 'more.\"', 'arms..', 'crap.', 'blind,', 'UN-B-REATHABLE!', 'Tasted', 'won', 'lost?', 'instrument', 'exist.', 'corny', 'spectacles', 'peaceful', 'Martin', 'pointless.', 'tie?', 'Gainesborough?', 'wet', 'later!', 'hare-line.', 'screaming', 'Rank:', 'April', '1930s.', 'Cinderella', 'stand', 'he', 'me)', 'unlawful', 'i', 'chummy', \"don't\", 'king', 'original)', 'personally', \"It's\", 'Mr.', \"vegetarian's\", 'erascist.', \"^zoidberg's\", 'on!', 'jeans', 'caves', 'woohoo!', 'duckdoo', \"Spector's\", 'slide.', 'firecracker', 'hair?', 'Backgroud', 'laugh?', 'up?!', 'week.', 'Id', 'llama', 'Counterfitz', 'Alpaca', 'house.\"', 'worker', 'epiphanies\"', 'letter?', 'sleep', 'tad', 'wrong', 'owner', 'one,', 'champion', 'Polar', 'bay?', 'Aww,', 'flags', 'friends', 'myself.', 'bunnies', \"Lovecraft's\", 'ching', 'picture', 'pretzels..', 'hospitalized,', '--', 'naan', 'hero.', 'Eyore', '&gt;Yes', 'while', 'Original', 'chaim-lich', 'escaped', '[', 'concentrate.', 'bottom', 'iSurfer', 'horned', 'two-tired.', 'Virginia', 'grate.\"', 'eat?', 'have', 'shelf', 'buried', 'icecream?', '\"Philately', 'collects', 'escargot!\"', 'based', 'old.', 'alllllllllmond](https://www.youtube.com/watch?v=fB63ztKnGvo&amp;feature=youtu.be&amp;t=37s)', '\"No', 'Botox', 'lifts', 'rash', 'FOUR!', 'dress', 'beer...', 'dd/mm/yy', 'di', 'out;', '\"It\\'s', 'year-old.', 'standing', 'accidentally', 'sod,', \"'ran';\", 'of', 'kid', 'dragon', 'Tunes?', 'ear', 'admire', 'an-arc-ist.', 'guards', 'pool', 'plant', 'toilets', 'wot', 'Frankenstein', 'America', 'moosecut!', 'gold?', 'dinosaur', 'first', 'stalking', 'follicle?', 'ss', 'race', 'parents,', 'Antennas', 'living', 'ba-na-na-naaaaa', 'weak...', 'flowers??', 'Simple', '8yo,', 'bovine', 'hopeless', 'sausage...', 'friends.', '\"Why', '\"olive', 'lot!', 'GRAAAIIINSSS!', 'language,', 'fortune', 'used', 'Fish', 'married?', 'cool.', 'cloche', 'bird.', 'nightmares', 'Fruit', 'holes', 'signal.', 'concrete', 'Elementary', 'orange', 'Running', 'margaritas!', 'Johnny', 'wings?', 'thesis', '\"Yeah,', '28', 'wearing', 'Satin!', 'unable', '\"grounds\"', 'Brazillionaire!', 'hammer', 'road', 'goldie!', 'Taylor', '...hands', 'meeting', 'large!', 'bill', '\"Aw', 'seat', 'odors.\"', 'addiction', 'brand', 'cruise', 'Secretary', 'Prince', 'her.', 'bed', 'type', '/r/jokes)', 'Cracks', 'boat\".', 'electronic', 'team?', 'Florida', 'roll.', 'Refresh', 'off', '\"L\"', \"Should've\", 'riveting.', 'beach', 'tank', 'surgeon.', 'supermarket', 'board', 'everything?', 'tangles?', 'carrots?', 'plaque...', 'troo', 'long?', 'Starry', 'there?', 'describe', 'Starburst!', 'Caesar', 'Denim', 'time.', 'someone', 'Hutus', 'manager', 'silly', 'walk', 'Jesus', '-but', 'Tooth', 'Enough', 'grows', 'headphones', 'comb.', 'BEING', 'walk.', 'gotta', 'memory', 'difference', 'money', 'think.', 'kick', 'company', 'life.', 'problems', 'confuse', 'account...', 'Ohm.', 'tired,', 'chickens?', 'up,', 'reason', 'snowman!', 'Nickelback', 'patty!', 'Addition)', \"'em!\", 'mop', 'wayward', 'Rabbit', 'shell!', 'There', '-\"Yes', 'chop.*', 'gossip?', 'cannibals', 'Cantaloupe.', 'Need', 'tend', \"ain't\", 'father', 'hunt', 'Sedan.', 'lawnmower', 'parrot?', 'programming', 'http://en.reddit.com/r/AskReddit/comments/1auxsf/what_are_some_funny_scientific_jokes_that_you_know/', '\"Our', 'beer?', 'garden?', 'fan', 'Bear', 'exhausted.', 'nose-feratu!', 'joke!:', 'replies', 'meowtin', 'worry', 'Trump?', 'Saitama', 'him?', 'manager?', 'uncle', 'opposite', 'lifeguard', 'walks?', 'events.', 'pigment', 'peanut', 'Prov-alone!', 'terminal', 'legs', 'archeologists', 'nervous', '\"well', 'https://www.youtube.com/watch?v=Lvlj1u9S258', 'Sun', 'FLICK', 'disability,', 'lame,', 'Tanks.', 'mechanic', 'Flabio', 'New', 'fence?', 'host', '**Joe:**', 'class.', 'Iraq?', 'paw.\"', 'remedy?', 'Windows!', 'suicide?', 'PARSLEYMONIOUS', 'destruction!', 'productive', 'playlist', 'NO', 'metrics:**', 'degree.\"', 'fish.', 'fsh', 'Coincidence', 'haircut', '***\"How\\'s', 'apple', \"I'd\", 'Pokemon', 'on', 'pause', 'cat?', 'lady', 'conditioning.', 'AskReddit!', 'mop.', 'others', 'fly.', 'family...', 'German', 'beach.', '\"Hey!', 'owned', 'expert.', 'losing', 'Yoda', \"shouldn't\", 'donation', 'lady.', 'students', 'listen', 'traveler', 'then,', 'Jones?', 'whole', 'deforestation', 'video', 'fisherman', 'Lambinated!', 'dance?', 'udder!', 'buddhist', 'wake', 'fun', 'selves', 'beach...', 'dog', 'ghost', 'Block', 'recycled.', \"wouldn't\", 'Suspoonders!', 'am', 'Russian?', 'humor.', 'city', 'geese', \"atheist's\", 'cliff...', 'tail,', 'erase', 'electron.', 'ways', 'ship.', 'Why?', 'kleenex', 'antenna.', '/r/tumblr', 'sheep?', \"Mis'ser\", 'http://www.youtube.com/watch?v=xv3gK2bmkAk&amp;feature=related', \"tryin'!\", 'bread?', 'can?', 'collided.', 'lose', 'Moroccan', 'worked', 'scrubbing!!!', 'medium', 'Farts.', 'Ahab', 'easy', 'evidence?', 'planner?', 'not).', 'poor', 'coffee?', 'grass', 'Broncos', '2', 'Religious', 'yep,', 'meters', 'Spud.', 'oooh', 'Penne', 'Dell', 'wont', 'Fax.', 'baby.', 'realized', 'mugged.', '8?', 'until', 'farming', 'wolf', 'vote', 'properly.', 'Bulls', 'animal?', 'government?', 'legal', 'getting', \"Mustard's\", 'resuscitates', 'loneliest?', 'Dr', 'buddy?', \"Pope's\", 'Even', 'camp', 'shorts?', 'versions?', 'torture', 'heavier', 'And', 'First', 'Transformer...', 'St.', \"How'd\", 'Nuremberg', 'Taco', 'ribbeting!', 'Juan,', 'Taffy.', 'This', 'clowns?', 'denim,', ':P', 'finger', 'Almonds', 'Four', 'villains', 'Perseid', 'wrote', 'bar,', 'job', 'shiftless.', 'storm', 'falls', 'horses', 'sweet\"', 'destroy', 'bedtime!](http://www.reddit.com/r/3amjokes/comments/1y8d67/what_did_the_mama_cow_say_to_the_baby_cow/)', 'a...', 'game?', 'rod', 'ceiling...', '(lethargy)', 'Jordan', 'shoes!', 'girl', 'lure', 'Joke!', 'windmills?', '\"A\"', \"...it's\", 'recovered.', 'Nothing.', 'slowly', 'raisin', 'bulb?', 'invite', 'letters!', 'Parsley', 'Sikh', 'store...', 'morning', 'beans,\"', 'Steak', 'ruff', 'CUT', 'running', 'violins!', 'private?', 'Frank', 'way', 'Yazidis.\"', 'ruining', 'milk?', 'too.', 'sank,', 'sticking', \"Han's\", 'girlfriend?', 'reading', 'Well,', 'GHOST!!!', 'Holmes', 'patient?', 'barber?', 'fit', 'often', 'society', 'tired', 'Joke?', 'body.', '(via', 'earth', 'scary?!', 'lightly.', 'begins', 'assaulted', 'pants!!!_', 'unwritten', '.....', 'carrot.', 'nation', 'opening', 'England', 'hares!', 'arrested...', '(X-post', 'is?', 'marker', 'yodeling!', 'hawk', 'Broom', 'spider', 'open', 'function?', 'multiply', 'BONOPOLY', 'use', 'funeral.', 'Sesame', 'essay.', 'Knock...', 'dillos.', 'barbed', 'beef!', 'Jim', 'horrorble.', 'species', 'includes', 'Belt', 'graphs!', 'pencil', 'quadriplegic', 'teller', 'Carrot', 'laundry', '\"Curiosity\".', 'Aonther', 'killed', 'Buffalo', 'ours.', 'live', 'much', 'into', 'builder', 'baseball?', 'piano,', 'fridge?', 'baseball', 'hubcap?', '(foreign)-hand.', 'Roses', \"giraffes'\", 'damn?', \"boards.'\", 'fired?', 'melted', 'Brazilian....', 'countries', 'searches.', 'minds', 'classes?', 'wire?', '555-bottom-feeders.', 'cross.', 'speed?', 'mysterious', 'Course', 'Egyptian', '^this', \"'Illinois\", 'Necronomnomnomicon.', 'roo', 'song', 'snow...', 'entailed', '2.Knock', 'injured', 'con-artist', 'being', 'toast.', 'E,', 'List', 'Saiyans', \"that's\", 'Carrion', \"you'll\", 'eating', 'Killer?', 'Greece...', 'hanging', 'dock!', 'limited', 'makes', 'cent', 'confronted,', 'Ask', 'flop', 'barman,', 'agent?', 'drum', \"Smith's\", 'excel', 'Notes.\"', 'arrested?', '\"Is', 'ones', 'courtesy', 'taste', 'bread...', 'lightbulbs?', 'playback', 'fight', 'contracts.', 'rise', \"couldn't\", 'well', 'doll,', 'American', '[x-post', \"doesn't\", 'joke)', 'rip', '5)', 'velcrows.', 'Second', 'anything,', 'east', 'Savage!!!!!', 'internal', 'porpoise.', 'sharp,', 'Seoul.', 'want-ads.', 'ribbit!!](https://www.youtube.com/watch?v=CYkDxsaHlkg)', 'X', 'miner.', 'mud!', 'half-way?', 'Mrs.', 'Russians', 'lights', 'currency?', 'fish!', 'so.', \"chia'd.\", 'hard', 'articles', 'r/jokes:', '\"Steamed', 'you.', 'Cumference.', 'Utter', 'them!', 'If', 'WHAT', 'then!\"', 'growing', 'friend', 'Leaves.', '8,', 'pushed', '13', 'geomeforest.', '8).', '34th', 'symbol', 'all.', 'Blue', 'KNOCK', 'literally,', 'sermon', 'flies.', \"He's\", \"whale's\", 'head', '*the', 'eggs?', \"Medusa's\", 'Pikachu', 'toilets!\"', 'automatics?', 'Bacon', 'criticizing', 'lonely?', 'honey', 'owns', \"Roamin'!\", 'support', 'stocked', 'guessed', 'State', 'cheep.\"', 'beans.\"', 'knock...', 'send', 'chirping', '[OC]', 'Bowl?', \"deez'll\", 'drinks?', 'comedy', 'Econoclasts.', 'allergic', 'stewicide', 'Nigeria,', 'forget', 'Boy', 'keeps', 'TV', 'island', 'whoa.', 'Intruder', 'library', 'cloud.', 'make', 'enough', 'monastery', 'balloon?', '12', 'Windsor', 'Thursday?', \"'er\", 'someone).', 'jar', 'folded', 'bakes', 'burrito', 'squid', 'League?', 'Roller', 'basketball', 'defeated.', 'recruiters', 'balloon', 'tickets', 'important', 'eyebrows', 'machine...', 'smoker', 'Law', 'experiment', 'boots?', 'naybor', 'stopped', '\"go', 'software', '...baseball', 'mucus', 'foreign', 'urge', 'His', 'strikes.', 'When', '\"What', 'Well', 'fair', 'Dogs', 'cranky?', 'socialist', 'exaggerate...', 'chess?', '6', 'shaving', 'guns!\"', '7?', 'mine', '***Take', 'Pelican', 'beer.', 'Monk', 'asked,', 'jealous', 'purple,', 'other...', 'Jihadist', 'guitar?', 'pie.', 'jokes)', 'antipasto', \"'Control\", 'promoted', 'pile', 'bold', 'likely', 'wheat', 'shin?', 'Amazon', 'stopping', 'tell', 'cliff.', '\"Papal', 'middle?', 'indeed,', 'today.', 'disturbing', 'Suture', 'scientific', 'teller.', 'flying', 'following', 'Audiophiles', 'year', 'feathers?', 'opossum', 'pretend', 'says,', 'mouse', 'bug', 'pick', 'raccoonteur.', 'distance', 'approval?', '\"Getting', 'http://www.reddit.com/r/AskReddit/comments/zrotp/whats_the_best_clean_joke_you_know/', 'breakfast.', 'songs', 'dime?', 'strawberry', 'Hard', 'zeroes', 'planed', 'keyboard?', '\"Nope\"', 'mineshaft?', 'My', 'force', 'pissed', 'safe', 'diagnose', 'estimate', 'blasfemur.', 'barbarian', 'penguin', 'form', 'problem', 'every', 'small', 'Dolls...', 'punch.', 'cheetahs!', 'Vlasic', 'coming', \"Dunkin'\", 'telling', 'hare', 'Europe', 'bald', 'Hi-Def', 'Lean', 'piano?', 'foreman', 'bread', 'drizzle.', 'sign', 'rhino?', 'faintest', 'mayor', 'unsanitary.', 'Russia:', '\"Can', 'rung.', 'IHOP!', 'hotdogs', 'nose', 'started', 'suits', 'MAGA', '\"woof,', \"sheep's\", 'ivy', 'field,', 'Loaf', 'Chucky.', 'Aurochnophobia.', 'Jersey', 'air.', 'group', 'They', 'roll', 'league', 'ramifications!', 'around?', 'great', 'cherry', 'butt.', 'shake', 'musical', 'pants.', 'Wawa', 'cat', 'general', '*p*', 'chicken?', 'Adrienne', 'Twerky!', 'mathematics?', 'key', 'chicken', 'ships.', 'topical.', '\"What\\'s', 'chase...', 'while.', 'hat.', '&lt;blank', 'native', 'grater', 'watch.', 'conditioned', 'original!', \"who's\", 'Ever', 'Groot.', 'pig', 'yolks!', 'officer', 'Schooner', 'Stew!', 'leave', 'brush', 'hat', 'fatty', 'Moo!', 'sink,', 'name', 'catching', 'Braille', 'food?', 'hippie', 'burger', 'backwards?', 'Waldo...', 'math...', 'Sigmund', 'drunk.', 'Broken', 'announces:', 'Milton', 'lemons.', 'psychic?', 'earrings?', 'left.', '[Man,', 'oath', '**Dances', 'Around', 'Con', 'cost?', 'Investigator!', 'Vine**', 'ele-fence', 'quack,', 'NapKin', 'classic:', 'gotten', 'mare?', '..................', 'roast', '[Credit', 'Red', 'language', 'round.', '\"Disney', '2.', 'giveaway!', '\"Yep,', 'Clooney', 'Job', 'hit', 'beef,', 'work', 'lipstick?', 'cymbal', 'footprints', 'Marie', 'so', 'Thai', '/r/cleanjokes', 'summer.', 'Sean', 'hundred', 'Pop.', \"I'll\", '...engage', 'sparrow', 'terror.', 'log', 'life?', 'Simpsons', 'here', 'Russia,', 'famous?', 'Protect', 'experience.', 'ramsey', 'play', \"can't\", 'beginning', 'hamburger', 'pillowcase?', '*Still*', 'call', 'holy', 'dumb...', 'suffer', 'artists', 'clairvoyants', '!!!!', '\"Back', 'said:', 'thirty', 'Tan?', 'born?', 'has', 'whenever', 'thats', 'learnt', 'vegan', 'cashew', 'in:', 'crocodile', 'Land', 'start', 'locked', 'lays', 'lettuce?', '/r/zelda?', 'paper?', 'leaves', 'charged', 'Take', 'robots.', 'ladder', 'money?', 'section?', 'dyslexia.', 'moostard', '^sombrero', 'secede?\"', 'republishing', 'platform', 'butt', 'jam.', 'Two-Baaas.', 'plus.', 'yam.', 'took', 'boxes', 'Denial.', 'inside?', 'Rock', 'because', 'Mainstream.', 'middle-class,', 'amount', 'ca-lip', 'them', \"y'all\", 'Loafers.', 'tea?', 'maid', 'crabby!', 'Demetri', 'Jeans', 'knob', 'VIII', 'website', 'AM', 'Beer.', 'Irish', 'oiled', 'Science', 'laced', 'underwater', 'Knock,', 'atheist', 'Very', 'blood?', 'Nothing,', 'protective', 'figurine', 'ask', 'flu?', 'grandfather...', 'China', 'order', 'dubious', 'Chef', 'cheese!', 'clerk', 'doors,', 'Her', 'itheburg.', 'usually', 'catch!', 'wonder:', 'bee', 'ran', 'Han', '\"Are', '1080i.', 'cheer', 'fried', 'cracks', 'woods', 'race.', '(naybor', 'control?', 'per-cushion.', 'push', 'haunted', 'pool?', 'swam', 'Butterfly', 'task', 'school?', 'chimps', 'sing?', 'slaves', 'butter', 'snakes', 'drown?', 'Soviet', 'stolen', 'First,', 'bullet', 'cookbook?', 'soap-', \"we're\", 'best', 'Shakespeare', 'selling', '*Swift*.', 'checks', 'service', 'Dam(n)', 'Yourself', 'both', 'dino-snore.', 'Where', 'insomnia?', 'XD', 'place', 'Cows', 'Prof.', 'underwear', 'Rocks!', 'driveway?', 'gerMANY', 'spend', 'Twisted', 'Impasta', 'refrigerator', 'tomato*', 'napkin.', 'flower', 'robin', 'ear?', 'neutrinos', 'intestine.', 'cousin', '...in', \"*They're\", 'spa-getti.', 'say,', 'urinate?', 'Superman', 'course,', 'joke.', 'dramatic', 'tents', 'high.', 'tumor', 'blushing?', 'quit', 'lack-toes', 'glass', 'LED', 'bone-setter?', 'followers', 'collector', 'Rubber-Toe!', 'KNOCK!', 'Chicken', 'machine', 'hominy.', 'irregularly,', 'guide?', 'keep', 'is!', 'up-who?', 'good.', 'Sir', 'other,', 'than', 'credit', 'goldfish?', 'once', 'bad,', 'for,', 'yesterday', 'orbyte', 'breed', 'Other', 'Pharaoh...', 'silent', 'no', 'boat?', 'spock', 'crisis?', '\"They\\'ve', '*She', 'hurts\"', 'needs', 'Jamaal.', 'sub-woofer', 'dye', 'contortionists', 'travelers', 'explosion?', 'bakeries?', 'wind?', 'announcement', 'Sorry,', 'unwieldy', 'Switzerland?', 'clean', 'bag', 'thieves', 'sailors...', 'way.', 'Service.', 'ointment', 'man', 'trial?', 'farts.', 'rubber', 'happen', 'Echosystems.', 'boars', \"Isn't\", 'genes!', 'todays', 'earing', 'control', '&gt;', 'jumping', 'rehab?', 'peoples', 'runs', 'happy', 'used,', 'steal', 'heavy', 'handed', 'rabbits?', 'upset?', 'Boo!!', ':-)', 'professional', 'meet', '\"Shrank', 'remember:', 'Gets', 'functioning.', 'cover', 'singer?', 'fitter\"', 'deer.', 'hops', 'visit', 'mosquito~', 'fact', 'glass?', 'factory?', 'hill!', 'joint?', 'large', 'mourning', 'goals', 'doctor:', \"We've\", 'quack\"', 'eggs!', '\"Four', 'swallow', 'sentence...', 'melon', 'Ketchup!', 'wonton', 'Thunderpants!', 'game...', 'little', 'asks:', 'laugh.', 'ants.', 'source?', \"ISIL's\", 'sandals?', 'security', 'prize?', 'novocaine', 'bran', 'wine', '\"May', 'Un', 'tasted', 'dome?', 'monkey.', 'Sriracha.*', 'peanuts', 'began', '...to', 'conversational', \"one's\", 'cited', 'pumpkin', 'toe', 'air?', 'porcupine?', \"it's\", 'Talking', 'cats', 'love', 'road...', 'refrigerator?', 'leggs.', 'avoided', '\"We', '...next', 'still', 'Mints', 'happens', 'hoping', 'wreath', 'pointless?', 'King,', 'too!', 'BE', 'Moose?', 'But', 'up.\"', 'Yeah,', 'Laughing', 'police', 'content!!!!', '*dodges', 'knows.', 'Pilgrims.', '\"do', 'bovine.', 'How', 'me...', 'eye', 'stadium)', 'you,', 'was', 'dessert?', 'laptop?', 'V.', '10', '\"who\"!', '\"Moo!\"', 'headed', 'pots', \"You've\", 'locomotive!', 'eyes,', 'Japanese', 'transcend', 'Endor?', 'muffler?', 'pitcher', 'y.o.]', 'around*', 'disappointed.', 'red?', 'From', 'County', 'oyster', 'Chardaneiiiiiiggghhhhh', 'Diesel?', 'credit,', 'dreams.', 'oh?', 'Catholic!', 'Book,', 'pterodactyl', 'scratch.', 'kid.', 'next', 'stegosaurus', 'ABCDEFGHIJKMNOPQRSTUVWXYZ', '1500', 'seen', \"Year's\", 'finding', 'straight', 'top', 'cups', 'over.', 'but....', '...He', 'that...', 'cold.', 'Is', 'randomly', 'Roman', '(Seriously.)', 'sturgeon', 'name?', 'loads', 'other', 'score?', 'Myself', 'imagination!', 'So', 'in?\"', 'Magic', 'ended', 'puddle!', 'operates', 'reported', 'longest', 'please.', 'skelleton', \"She's\", 'buck?', 'oak', 'meteor.', 'angry', 'letters', 'give', 'turns', 'ten', 'celebrity', '(p.s.', 'Everything', 'legislation.', 'citing', 'soldier', 'cheesy', 'shoelaces?', 'lonely', 'news!', 'chew!\"', 'Star', 'eats', 'bank.', 'Superman?', 'Official.', 'person.', 'huile', 'easily', 'classify', 'Lucky', '...across', 'field.', 'had......', 'follow', 'homework?', 'joke?', 'it,', '...its', 'finally', 'yesterday...', 'held', 'Cogito', 'dinner?', '#ThugLife', 'socrates', 'kidnapping', 'Protestants', 'sects', 'psychic', 'Cleveland', 'easter', 'golfer,', 'know.', 'Yeah', '\"Ow\"', 'celebrating?', 'Ulta', 'checked', 'developers', 'Hampshire', 'We', 'grate', 'Unique', 'mom', 'fix', 'Me', 'cookies?', '1:Uh,', 'desert', 'Unless', 'gin.....and', 'listening', 'Math', '*(Phonetically-ish)*', 'Teletubbie.', 'Wanna', 'horn?\"', 'LOCK', 'polar', '7-up', 'Astana-shing', '*problems!*', '0', 'yet!', 'saucy.', 'poem', 'Tennish', \"you'd\", 'Sh...', 'office.', 'dollar', 'nose...', 'golfer', '$0.45?', 'Smith', 'hat*', 'the', 'gamers', 'up.', 'pilot', 'husband...', 'Greek', '*Frank', 'me,', 'tried', 'prison?', 'bottles,', 'walking', '55', 'U', \"hairdresser's\", 'Epcot)', 'conversation', 'special', 'pollen', 'pasta', 'appendix', 'Gar', 'mac', '*****SOMBRERO-VER', 'only', 'ships!', 'ba-dum', '...Carrying', 'sit', 'not', 'Geometry', 'Indian', 'eggs', 'shorts.', 'pint', 'tutor', 'cats?', 'mathematicians', 'difference...', 'Friday.', 'pizza', 'physicist', 'advocates', 'Earth', 'youre', 'such', '...then', 'Beef', '123456', 'DOGerpillars!', 'Cleaver.\"', 'windows', 'funny!', 'yard', 'U-turns!', 'is,', 'fitness', 'rocks?', 'Z', 'wreck.', 'off.', 'monocles', '\"ISIL', 'bells?', '(Quoted', \"they've\", 'drawers!', 'nightly', 'our', 'D.', 'other?', 'magic', 'BACH*', 'Hollandaise.', 'Tri-tip.', 'laptop', 'smelly', 'sick.', 'ingesting', 'lamp', 'telephone', 'hopped', '1000', 'put', '*A', 'r/The_Donald', 'fly!', 'few', 'self-deprecating', 'Pointless!', 'Tryptophanatic.', 'short', 'appendix.', 'documentary', 'crashing', 'opened', \"wasn't\", 'zoo...', 'Why,', 'neutrino', 'drink?', 'objects', 'DIABETES', 'n', 'met', 'ten-tickles', 'salons', 'parties?', 'chemical', 'quack.\"', 'could', 'Watson.\"', 'pieces', 'drag', 'skydive?', 'Japanese?', 'apart.', 'headlines!', \"ol'\", 'barman', 'carrot', 'woman', 'set!', 'cannellini!', 'modest.', 'flies', 'afterwards', 'Cock-a-doodle-moo!', 'support.', 'beauty', 'belong', 'kicked', 'blood...', 'quite', 'clear', 'can.', \"Let's\", 'accident..', \"you've\", 'Jones]', 'knotsies!', 'koalas', 'arch', '90', 'ball.*', 'spots?', 'each', 'chips', 'Never', 'smartest', 'excellent', 'revised', '\"No.', 'T-shirt', 'insect?', 'round', 'crisis', 'knock.', 'Carta', 'slide', 'Friday', 'care', 'why', 'Picard', 'Mummy', 'knack', 'amateur', 'cables?', 'intolerant.', 'obsessed', 'chew,', 'sumos', 'tripping', '2:**', 'sucks', 'brains?', '*Peas', 'playground?', 'Nice', 'chloroform', 'Taun-Taun?', 'floating', 'response?', '*Al', 'couldnt', 'away,', 'nation!', 'hipster', 'ear.', 'terrible,', 'moon', 'rope', '2)', '\"...Why', 'searching', 'shift', 'grease.', 'school,', 'Fart', \"martian's\", 'MOLD', 'dermatologist', 'better,', 'bar.', 'trees!', 'got', 'Bunny', 'hula', 'everybody', 'same...', 'cans', 'Halfway.', 'chocolate?', 'nice', '\"I', 'Peach!', 'economic', 'pebble', 'Ben', 'merely', 'recordings.', 'Please', 'hotel', 'sluggish.', 'oooooohms.', 'havent', \"wouldn't?\", 'ton,', 'washed', 'niece', '7,', 'what', 'mint', 'Ewoks', 'funerals', 'this,', 'throw', 'Yeah.', 'boogy', 'enters', 'science', 'mighty', 'Mario', 'humps?', 'hotel...', 'EDIT:', 'idea?', 'Mexico', 'is.', '*body*', 'stories?', \"Washington's\", 'Root', 'AN', \"Frankenstein's\", 'it', 'cowculator.', 'Somewhere', 'Its', 'fun?', 'armies?', 'fox', 'drank', 'weirdos', 'going?', 'curtains', '\"Control', 'termite', 'mean?', \"''Did\", \"person's\", 'Li[e]-lacs!', 'salad', '14days...', 'tortoise', 'prince', 'Wrigley,', 'age', 'fun!', 'Eclipse', 'Internet', '1.Knock', 'traffic', 'weed', 'paint', 'Clinton', 'paws.', 'terrifying', 'Bach', 'lies', 'passengers', 'owl', 'thousands', 'horse.', 'table!', 'truck', 'rare', 'sleep,', 'fallen', 'ham', 'knee', \"she's\", 'pt', 'field', 'plan', 'Air.', 'trees?', 'types', 'plate', 'buccaneer!', 'Borax?', 'Giant', 'deep.', 'hoarse.', 'sneaker.', 'flight', 'flaked', 'mortgage', 'BOOOOgers.', 'ocean?', 'services?', 'boxer', 'overpriced', 'pigs', 'bored', 'unpainted', 'Washed', 'customer', 'Clown', 'a-rye', 'various', 'Noah', 'thrown', '...or', 'tiers.', '**Tank', 'biggest', 'kind', 'above', 'crypt', \"Pavlov's\", '~~Five', 'Sorry', 'affect', 'body?', 'club?', 'doll', 'https://www.youtube.com/watch?v=kNt-aTq0hxM', 'closed', 'bus', 'font?', 'College', 'duck.', 'lettuce', 'Gravity', 'vulture?', 'funny.', 'whales?', 'Ohio', 'cop', 'well,', 'drivers', 'short,', 'cello.', 'Congratulation', 'flew', 'Gordon', 'koalafications.', 'guests', 'gave', 'changed', \"traveler's\", 'racecar', 'Sports:', \"29's\", 'down.\"', 'good', 'drying', 'starting', 'nipples?', 'girl,', 'Bicycle', 'thanksgiving', 'perfect', 'I', 'impasta.', 'consuming,', 'Rex!', 'Subreddit', 'Solo', 'ATM', 'Tony', 'tense!', \"vampire's\", 'Bros.', 'too', 'burned', '$10', 'Batman.', 'pelican', 'river', 'Horseman', 'hilarious!', 'reasons.', 'fruit?', 'E.', 'friends,', 'little...', 'sold', 'So,', 'giraffes', 'IRL](/spoiler)', 'sleeping...', \"waggin'\", 'doctor?', 'sleeper.', 'bring', 'impasta!', 'crashed', 'anti-gravity.', 'TIED-UP!***', 'Cos', 'cleaning', 'sidebar,', \"there's\", '32.', 'far,', 'movie', 'banned?', \"friend's\", 'smart', 'knows...', 'morning...', \"duckdoo?'\", 'seeing', 'hands!\"', 'lawyer', 'Marx', 'pretty', 'said...', 'win', 'fly...', 'Subscribers:', 'goose', 'cut', 'Sherlock', 'mama', 'become', \"Valentine's\", 'iron', 'Birthday', 'Street', 'stingy', 'dente*,', 'then', 'Hungarian', 'Belgium?', 'lairs.', 'passive', 'petition', 'turnover.', 'passing', 'atom', 'breakfast', 'yours', 'crisis.', 'wanted,', 'dates.', 'Sure,', 'designers?', 'corny,', 'Eye-fi.', '\"Say', 'breathe', 'most', 'envelope', 'delays', 'GROUT', 'using', 'corn?', 'Unlawful', 'lazy', 'when', 'sure', 'today...', 'why.', 'Saus', 'never', 'guys,', 'galaxies', 'Country', 'flatterer.', 'Lepre-khaaaaannnnn!!!!!', 'watch', 'misled.', '\"Cliff', 'dying', '\"beans\"', 'dressing!', 'bank?', 'hard.', 'relationship.', 'Vector', \"(that's\", 'House', 'milk', 'study', 'private', 'was,', '*Then', 'fire', 'street....', 'trousers', 'paper', 'inside', 'cavator', \"Fo'\", 'lover?', 'involving', 'joining', 'beef.', 'roads', 'tailpipe', 'life.\"', 'cauldron.', 'come', 'Bubbles', 'printer', 'Engineers', 'marries', 'spacious', 'hunter', 'FLAC', 'WAH', \"dad's\", 'mouthwash?', 'bat', 'umbrella?', 'piece', 'a-singing.', 'suckers', \"grandpa's\", 'clock...', 'noisy', 'Bonnie', 'manholes', 'Bird', 'stuff', ':O', 'Titanic?', 'Passwords', 'look', 'ground?', \"Gump's\", 'wear', 'sprinkled', 'year.', 'Sam', 'Then', 'Meat', 'name...', 'Anything', 'magazines', 'strong', 'bean', 'mother.', '^in', 'halloweenie', \"Sun's\", 'shop', 'reptile', 'legs.', 'mussel.', 'superhuman', 'child?', 'knew', 'shrank', 'letters.', 'pilaf.', 'electrical', 'head?', 'arms', 'bride.', 'calm', '(age', 'season', 'trans-sisters.', 'time', 'games?', 'Dalai', 'Branson,', '[OC', 'one.\"', 'It', 'labracadabrador.', '\"My', 'lover.', 'unemployed', 'belt?', 'A', 'West', 'Autobot', 'sail', 'upon', 'warm', 'smell', 'psychologist?', 'Check.', '45.', 'HashTag!', 'cup.', 'job...', '*knock', 'without', 'terminator', 'canes...', 'young', 'fault.', 'Christmas', 'lion', 'most?', 'Schwarzenegger', '[this](http://www.reddit.com/r/funny/comments/xdp4k/the_gaydar/c5lnkep)', 'joke!', '(Credit', 'Beef!', 'towel.', 'like?\"', 'beater', 'Vader', 'firequacker.', 'balloons', 'degrees.*', 'pub', 'married,', 'work.', 'jazz', 'American,', 'life', 'metal', '\"is', 'doctors', 'plane?', 'ally!', \"martini's\", '\"R\"', 'legless', 'percussion', 'gum', 'http://imgur.com/PKibj', 'GPS', 'Skeleton', 'minded...', 'Doughboy', 'tree.', 'mathematician?', 'TWO', 'cloth?\"', 'replied', 'checks.\"', 'line.', 'wholesaler.\"', 'name.', 'Child', 'cured!\"', '\"Hey,', 'View\".', 'Every', 'Marked', 'store!', 'Philippe', 'hose', 'technology', 'obvious.', 'knock', 'Anthony', \"d'olive\", 'punch', 'prose.', 'spy?', 'He', 'tune', 'is...', 'documents?', 'tents.', 'hotdog', 'end,', 'Killer.', 'iTunes', 'opportunity?', 'crabs', 'here?', 'in', 'Amonds', 'Black', 'buildup?', 'fluffy?', 'pencils...', 'shuffles.', 'quick', 'nothing', \"That's\", 'bigger.', 'incontinental', 'bear', 'problem:', 'Olympic', 'Snoop?', 'electrician', 'tank.', 'baristas?', 'puns,', 'know,', 'fin', 'mullet', 'thinner!', 'bother', 'heck', 'Propaganda', 'penguin?', 'geology', 'loaves', 'vegetables', 'time!\"', 'Myfunsalow', 'you', \"spectre's\", 'mate!', 'body', 'outweighed', 'brought', 'program', 'clips', 'jokes?', 'Bach,', 'filled', 'Yeah,the', '2\"X4\"\\'s', 'Quija', 'tie', 'cantaloupe.', 'medication.', 'Tell', 'Currently,', 'cactus', 'cowboy', 'faster-than-light', 'try?', 'list?', 'wind.', 'number.', 'woof,', 'camping', \"WHO'S\", 'now!', 'Headless', 'movement.', 'bread.', \"other's\", 'may', 'astronut.', 'Denver', 'boot', 'veloci-raptured.', 'http://www.reddit.com/r/pickle/comments/1a2xg8/next_attack_for_our_entire_army_march_12th_at_520/', 'student', 'Facebook', 'Miles', '\"Stay', 'surgeon', 'loaf', 'male', '27.', 'life!', 'universe', 'pineapple?', \"Where's\", 'pea', 'spitting', 'Proud', 'accomplice?', 'friendliest', 'Will', 'laughed', 'brilliant.', 'Rolling', 'who.', 'teeth?', 'Hootonium', 'Siberian', 'fret?', 'dumbest', 'article', 'law', 'Donkey', 'Coke', 'gymnast', 'more', 'Good', 'product', 'gig.', 'homemade', \"[It's\", 'overfill', 'spray.', 'Jedi', 'changing.', 'from?\"', 'crack', 'tragic', 'who?**', 'South?', 'Wanted\"', 'pretzels', 'point.', 'wanna', 'ducked.', 'Ground', 'sharp.', 'Chances.', 'Aretha', 'jawed', 'Thanks.', 'broken', 'ceremony', 'ribbon.', 'nickname', 'babies', 'light.\"', 'term', 'mud', 'celebrate', 'Roberto', 'Donut.', 'carry', 'teach', 'numbers', 'cardboard', 'approaching,', 'Seven', 'Tuffet?', 'over?', 'hop', 'obscure', \"raisin'\", 'lost', 'spelling,', 'overheads', 'crop', 'snap!', 'test?', 'Buh', \"it('s\", 'yarn', 'legs?', 'pointless.\"', 'cookies.', 'pet', 'dogs?', '...Then', 'Boo', 'shoes...', 'window?', 'RALSHMICOMN', 'friend.', 'ok', 'profanity?', '\"That', \"flippin'\", 'own', 'half?', 'slipper', 'rabbit', 'things', 'Lovecraft', 'later.', 'dis-ability.', 'fueled', 'preparing', 'matter', 'today,', 'yet.', 'speeches', 'sleeps', 'looks', 'choose,', 'Reinventing', 'object', 'caught', 'bar....', 'firecheif', 'garnish?', '2,246', 'Arnold', 'Combi', 'sore', 'lives', 'today!\"', 'before.\"', 'books', 'hes', 'minored', 'Freddy', 'hominy', 'Supereyore', 'charge.', 'cereal...', 'Elsa', 'Nine', 'people?', 'strip...', 'loud?\"', 'female', 'line', \"bowl's\", 'dry?', 'train?', 'tuna', 'Milestones', 'mistake.', '*Come', 'reggae', 'printer,', \"balloon's\", 'measure', 'heard', 'K.', 'typo.', 'Kansas', 'poker', 'Thanksgiving', 'daddy', 'ever.', 'justified.', 'restaurant?', 'Thanksgiving?', \"he'd\", 'throat?', 'dog?', 'Peas', 'Halloween', 'Faithbook', 'Reddit', 'Words', \"Today's\", 'it*.', 'mode', 'Wheat', 'them.\"', 'party', 'footwear', \"oven's\", 'store', 'good?', 'kent', 'lucky', 'tweetment', 'walks', 'forget,', 'epileptic', '5', 'receding', 'buffalo', 'intervention.', 'politics?', 'second', 'Looking', 'go', 'nihilists', '***ALL', 'kid?', 'son', 'snail', 'there.', 'faster', 'big', 'uses', 'Tom', 'cheet-ahs', 'dish', 'tends', 'metro-gnome....', 'beans', 'trash', 'golfing?', 'effeminate', 'Kurd', 'scary', 'stringed', 'check', 'hoes..', 'termination!', 'omg,', 'Brushing', 'Empire', 'internet', 'Monopoly,', 'now!)', 'fur?\"', 'bike', 'chance', 'loved', 'pencil?', 'park', 'essay', 'roof.', 'chess', 'war!', 'Walmart', 'gray', 'Joseph', '*butter*', 'Ten', 'Apple', 'knight?', 'Smiles', 'band?', 'back...', 'Train', 'skeleton', 'Fine', 'swollen', '\"we', 'know\"', 'movie?', 'huge', 'D', 'brother', 'Hexed', 'manhole', 'all', 'that', 'liters', 'minimalists', 'horrible', 'Overheated', 'Muenster.', 'brother.', 'write?', 'Crabs\".', 'minutes', 'deprecating', 'wait!', 'wheel', 'anorexic', 'aquarium?', 'octopus', 'vest!', 'wall?', 'Cause', 'may?', 'r/Jokes]', 'Knock', 'talent', 'Interrupting', 'would', 'elevator.', 'K.G.B.,', 'stepped', 'crabs?', 'sofas', 'brief', 'Forrest', 'dirty-', 'Blaze', 'call!\"', 'lack-cluster.', 'dessed', 'related?\"', 'guitar....', 'hair', \"astronaut's\", 'Eggs', 'streets', 'watched', 'several', 'pull', '4', 'music', 'daughter', 'who?', 'policeman', 'dish?', \"*I'LL\", '1', 'remote', 'drill?', 'Pencil', 'run', 'pickled', 'didnt', 'bat!', 'Holland?', 'third', 'dust.', \"baby's\", 'Fo-Drizzle', 'pulled?', 'afraid', 'impeached!', 'lacked', 'eek', 'shady', 'Disco', 'MoooooOOOOOOoooooooo', 'Paradox.', 'Cold', 'Vincent', 'accounts', 'boomerang', 'Moleskine', 'clothes.', 'Constipation?', 'Bert!', 'committing', 'girltree', 'broom', 'unthinkable?', 'WAKA', 'wh...', 'Atlantic', 'rated?', 'stake', 'noodle?', 'bitter', 'https://www.youtube.com/watch?v=rQegAi6d-MM', 'reek.', 'Typical', 'hamstring.', 'JKLMNOPQRST', 'baked', 'doesnt', 'play?', 'plane', 'pretending', 'http://i.imgur.com/EzT0Bkd.jpg)', \"Deen's\", 'cry?', 'technician.', '\"quack,', 'Panda-monium.', 'tender?\"', 'much?', '\"aaaah!', 'Wom.', 'drive', 'wives', 'Knock!', 'say.', 'store?', 'ba', 'penguin.', 'molasses', 'booger?', 'camo.', 'wall', 'turn', 'Hope', '[Through', 'dirty', 'spirits.', '999', 'kids', 'Dough', 'noodles', 'Act', 'those', '(the', 'music...', 'Thread', 'hate', 'interpretation', 'Potter', 'walked', ':).', 'mistakes...', 'Hugo.', 'Christmas!', 'Gastroenterologists', 'drown', 'listens', 'reddit', 'lefts', 'one?', 'sharks', 'watching', 'got:', ']', 'ticket', 'sex,', 'deciding', 'Scrabble', 'day.\"', 'sheep.', 'No?', 'positive!\"', 'jungle...', 'fish?', 'yam!\"', 'saved', 'practice.', '-I', 'rich', 'karma!', 'after', 'competing', 'shower', 'myself', 'War?', 'Dolphins', 'pepper?', 'someone.', 'night.', 'abroad', 'single', 'civilwar', '/r/3amjokes)', 'this)', 'donating', 'zoo', 'diving', 'create', 'Ohio?', 'center?', 'robots', 'ownership?', 'night', 'Graaaaains!', 'Times', 'passed', 'obstacles', 'note:', 'get', 'waddled', 'wedding', '...because', 'necks', '\"Alaska.\"', 'Lawsuits.', '37'}\n",
      "6925\n"
     ]
    }
   ],
   "source": [
    "data = readFile_csv(filename, header)\n",
    "print(len(data))\n",
    "# create a Counter for our data\n",
    "data_counter = Counter(data)\n",
    "# find all unique words\n",
    "unique_words = set(data)\n",
    "# print the list of unique words\n",
    "print(unique_words)\n",
    "# print the number of unique words\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3EX6QyOdNyH",
    "outputId": "1d63dffb-5c07-4768-b58c-f94fb341db98"
   },
   "outputs": [],
   "source": [
    "# create a word-to-index dictionary\n",
    "# create an index-to-word dictionary\n",
    "\n",
    "# find the indices in all words in the dataset\n",
    "# print the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nqlap56kUDOG"
   },
   "source": [
    "\n",
    "Our inputs will be the words of a chosen sequence length, and the outputs will be the next word.\n",
    "We want to predict the next word from the current sequence of words, so we will create sequences, which are groups of consecutive words.\n",
    "\n",
    "For example, consider the sentence: **What did the bartender say to the jumper cables?**. For a chosen sequence length of 4:\n",
    "- input sequence: ['What', 'did', 'the', 'bartender']\n",
    "- target sequence: ['did', 'the', 'bartender', 'say']\n",
    "The output sequence is always one time step ahead of the input, and the set of input and sequence sequence gives one data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCi7VYfXawj6",
    "outputId": "473f83a9-6e0b-4ef2-fc0d-4e33f1809cc3"
   },
   "outputs": [],
   "source": [
    "sequence_length= 4\n",
    "index = 0\n",
    "# transform the words into a tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLNTSQvXB-S0"
   },
   "source": [
    "## Creating a custom TensorDataset that allows us to perform the following:\n",
    "- Take as inputs a list of all words in our text\n",
    "- Generate a dictionary of all unique words and their counts, sorted in descending order\n",
    "- Return a sample of input and output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abra': 0, 'ca': 3, 'dabra': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 3, 2, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = zip([1,2,3],[\"t\",\"b\",\"a\",\"a\"])\n",
    "# print(list(a))\n",
    "# Counter([\"t\",\"b\",\"a\",\"a\"])\n",
    "# for i, l in enumerate(\"abracadabra\"):\n",
    "#     print(i, l)\n",
    "\n",
    "#list of words\n",
    "words = [\"abra\", \"ca\", \"dabra\", \"ca\"]\n",
    "\n",
    "# words to idx\n",
    "d = dict([(l,i) for i, l in enumerate(words)])\n",
    "print(d)\n",
    "\n",
    "# idx instead of words\n",
    "# idx_list = []\n",
    "# for i in words:\n",
    "#     idx_list.append(d[i])\n",
    "[d[i] for i in words]\n",
    "# print(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vVXHC-veU4Ih"
   },
   "outputs": [],
   "source": [
    "class WordsTensorDataset(TensorDataset):\n",
    "    def __init__(self, data_list, sequence_length=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_list (dictionary): A list of all the words in the file\n",
    "            sequence_length: the number of words in each input sample, and output sample\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data_list = data_list\n",
    "        self.unique_words = self.get_unique_words()\n",
    "\n",
    "        # create a dictionary of mappings of words to indices\n",
    "        word_to_idx = dict([(i,l) for i, l in enumerate(self.unique_words)])\n",
    "        \n",
    "        # create a dictionary of mappings of indices to words\n",
    "        idx_to_word = {v: k for k, v in word_to_idx.items()}\n",
    "\n",
    "        # return a list of the data with the words represented with their indices\n",
    "#         self.words_as_idxs = [idx_to_word[id] for id in self.data_list]\n",
    "        self.words_as_idxs = [idx_to_word[id] for id in self.unique_words]\n",
    "\n",
    "        \n",
    "    def get_unique_words(self):\n",
    "        return list(set(self.data_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_as_idxs) - self.sequence_length\n",
    "#         return len(data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample_input = torch.Tensor(self.words_as_idxs[idx:idx+self.sequence_length])\n",
    "        sample_output = torch.Tensor(self.words_as_idxs[idx+1:idx+self.sequence_length+1])\n",
    "\n",
    "        return sample_input, sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 1., 2., 3.]]), tensor([[1., 2., 3., 4.]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dataset = WordsTensorDataset(data)\n",
    "words_dataloader = DataLoader(words_dataset)\n",
    "next(iter(words_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsc5DqUO0LiB",
    "outputId": "d8677377-86f4-48c4-8e02-58242c61618f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dataset\n",
    "# use the dataset with a data loader\n",
    "\n",
    "# next(iter(words_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTiuEeIx5kgz"
   },
   "source": [
    "##Implementing a Text \"Generator\" Network with RNNs\n",
    "\n",
    "We will use the pytorch built-in Embedding for this exercise. The [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) enables us to encode words into a sort of look-up table. You will learn more about embeddings in the next lecture.\n",
    "\n",
    "To implement our RNN, we will start by creating an `nn.Module` that will represent a single RNN cell. This cell will update its hidden state by:\n",
    "\n",
    "- Applying a linear, fully connected layer to the cell input.\n",
    "- Applying a linear, fully connected layer to the previous hidden state.\n",
    "- Applying a non-linear activation to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o6roJP495iBi"
   },
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True, activation=\"tanh\"):\n",
    "        super(RNNCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        # select appropriate activation function\n",
    "        self.activation = nn.Tanh()\n",
    "        # create linear layer from input to hidden state\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size, bias=self.bias)\n",
    "        # create linear layer from previous to current hidden state\n",
    "        self.fc2 = nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n",
    "\n",
    "        # initialise the parameters\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # copy pasted\n",
    "        std = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, input, h):\n",
    "        # map from input to hidden state space\n",
    "        input = self.fc1(input)\n",
    "        \n",
    "        # map from previous to current hidden state space\n",
    "        h = self.fc2(h)\n",
    "        \n",
    "        # calculate new hidden state by applying activation\n",
    "        h = self.activation(input + h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-v-mXwloQOA"
   },
   "source": [
    "Once we have implemented a single RNN cell, we can write another `nn.Module` for our RNN network: it will contain one or more cells, concatenated forming multiple layers, and will apply the RNN cells to a given input sequence.\n",
    "\n",
    "The final output of the RNN will be obtained by applying a final fully connected layer to the last hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "SqRBxmtWqmxY"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, bias=False, activation='tanh'):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "\n",
    "        # create a list of modules\n",
    "        self.rnn_cells = nn.ModuleList()\n",
    "\n",
    "        # create each layer in the network\n",
    "        # take care when defining the input size of the first vs later layers\n",
    "        # (layers containing rnn cells?)\n",
    "        for i in range(self.num_layers):\n",
    "            self.rnn_cells.append(\n",
    "                RNNCell(\n",
    "                    self.input_size if num_layers == 0 else self.hidden_size,\n",
    "                    self.hidden_size, \n",
    "                    self.bias, \n",
    "                    activation\n",
    "                )\n",
    "            )\n",
    "        \n",
    "                \n",
    "        # create a final linear layer from hidden state to network output\n",
    "        self.h20 = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def init_hidden(self,  batch_size=1):\n",
    "        # initialise the hidden state\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size, requires_grad=False).to(device)\n",
    "\n",
    "    def forward(self, input, h0):\n",
    "        # Input of shape (batch_size, seqence length, input_size)\n",
    "        # Output of shape (batch_size, output_size)\n",
    "\n",
    "        outs = []\n",
    "        hidden = []\n",
    "        for layer in range(self.num_layers):\n",
    "            hidden.append(h0[layer, :, :])\n",
    "            batch_size = input.size(0)\n",
    "            step_size = input.size(1)\n",
    "        \n",
    "        # iterate over all elements in the sequence\n",
    "        for t in range(step_size):\n",
    "            # iterate over each layer\n",
    "            for layer in range(self.num_layers):\n",
    "                # apply each layer\n",
    "                if layer == 0:\n",
    "                    hidden_l = self.rnn_cells[layer](input[:, t, :], hidden[layer])\n",
    "                else:\n",
    "                    hidden_l = self.rnn_cells[layer](hidden[layer-1], hidden[layer])\n",
    "                # take care to apply the layer to the input or the\n",
    "                # previous hidden state depending on the layer number\n",
    "\n",
    "                # store the hidden state of each layer\n",
    "                hidden[layer] = hidden_l\n",
    "                \n",
    "            # the hidden state of the last layer needs to be recorded\n",
    "            # to be used in the output\n",
    "            outs.append(hidden_l)\n",
    "        # calculate output for each element in the sequence\n",
    "        out = torch.stack([self.h20(out) for out in outs], dim=1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTt-b0RJo-C7"
   },
   "source": [
    "We can now use out RNN network, together with the `nn.Embedding` to form our text-generating network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "o7m9Qg2Pj3Nw"
   },
   "outputs": [],
   "source": [
    "class RNN_GEN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, num_layers, num_unique_words):\n",
    "        super(RNN_GEN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_unique_words = num_unique_words\n",
    "\n",
    "        # add a nn.Embedding\n",
    "        self.embedding = nn.Embedding(self.num_unique_words, self.embedding_dim)\n",
    "        # add our RNN\n",
    "#         self.rnn = RNN(self.input_size, self.hidden_size, self.num_layers, self.unique_words, self.bias=False) \n",
    "        self.rnn = RNN(self.input_size, self.hidden_size, self.num_layers, self.unique_words, False) \n",
    "    def forward(self, x):\n",
    "        # initialise hidden state\n",
    "        batch_size = x.size(0)\n",
    "        # store the word embeddings\n",
    "        hidden = self.rnn.init_hidden(batch_size)\n",
    "        # apply the RNN\n",
    "        embedded = self.embedding(x)\n",
    "        output = self.rnn(embedded, hidden)\n",
    "        return output\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kngHA4W3il6P"
   },
   "source": [
    "Define the train and evaluate functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 1., 2., 3.]]), tensor([[1., 2., 3., 4.]])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(words_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_GEN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, num_layers, num_unique_words):\n",
    "        super(RNN_GEN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_unique_words = num_unique_words\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_unique_words, self.embedding_dim)                   # add a nn.Embedding\n",
    "        self.rnn = RNN(self.input_size, self.hidden_size, self.num_layers, self.num_unique_words,  # add our RNN\n",
    "                       bias=False, activation='tanh')\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.rnn.init_hidden(batch_size)      # initialise hidden state\n",
    "\n",
    "        embedded = self.embedding(x)                   # store the word embeddings\n",
    "        output = self.rnn(embedded, hidden)            # apply the RNN\n",
    "\n",
    "        return output\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_GEN(\n",
    "    input_size=4, \n",
    "    embedding_dim=4, \n",
    "    hidden_size=4, \n",
    "    num_layers=1, \n",
    "    num_unique_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_rnn_gen(model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      2\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters()), \n\u001b[1;32m      3\u001b[0m               criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(), \n\u001b[1;32m      4\u001b[0m               dataloader\u001b[38;5;241m=\u001b[39mwords_dataloader)\n",
      "Cell \u001b[0;32mIn[40], line 16\u001b[0m, in \u001b[0;36mtrain_rnn_gen\u001b[0;34m(model, optimizer, criterion, dataloader)\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# get output\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# compute the loss (change shape as crossentropy takes input as batch_size, number of classes, d1, d2, ...)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlds4p/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlds4p/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 20\u001b[0m, in \u001b[0;36mRNN_GEN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39minit_hidden(batch_size)      \u001b[38;5;66;03m# initialise hidden state\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)                   \u001b[38;5;66;03m# store the word embeddings\u001b[39;00m\n\u001b[1;32m     21\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(embedded, hidden)            \u001b[38;5;66;03m# apply the RNN\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/mlds4p/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlds4p/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlds4p/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlds4p/lib/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "train_rnn_gen(model=model, \n",
    "              optimizer=torch.optim.Adam(model.parameters()), \n",
    "              criterion=nn.CrossEntropyLoss(), \n",
    "              dataloader=words_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "XIdYTSP9eaIY"
   },
   "outputs": [],
   "source": [
    "def train_rnn_gen(model, optimizer, criterion, dataloader):\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "    # initialise the loss\n",
    "    train_loss = 0\n",
    "    # loop over dataset\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        print(X)\n",
    "        # send data to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # get output\n",
    "        \n",
    "        outputs = model(X)\n",
    "        # compute the loss (change shape as crossentropy takes input as batch_size, number of classes, d1, d2, ...)\n",
    "        \n",
    "        loss = criterion(outputs, y)\n",
    "        # backpropagate\n",
    "        loss.backwards()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    return train_loss/len(dataloader)\n",
    "\n",
    "\n",
    "def predict_rnn_gen(dataset, model, text, next_words=100):\n",
    "    # set model to evaluation mode\n",
    "\n",
    "    # loop over words\n",
    "        # take word from dataset and send to device\n",
    "        # compute output and hidden state\n",
    "\n",
    "        # take last output\n",
    "\n",
    "        # obtain probability vector for last output\n",
    "        # sample probability vector to get index in dataset\n",
    "\n",
    "        # get word corresponding to dataset\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnK0o1SAieM8"
   },
   "source": [
    "###Hyperparameters, model initialisation and the training loop  (Afternoon exercise)\n",
    "\n",
    "Let's train our network for a fixed number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "id": "22KIX_G2icxo",
    "outputId": "912994d8-6455-4249-b5a0-313758e56d21"
   },
   "outputs": [],
   "source": [
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = 128\n",
    "n_hidden = 128\n",
    "n_layers = 1\n",
    "embedding_dim = input_size\n",
    "n_unique_words = len(words_dataset.unique_words)\n",
    "batch_size = 256\n",
    "sequence_length = 4\n",
    "\n",
    "lr = 5e-3\n",
    "momentum = 0.5\n",
    "n_epochs = 20\n",
    "\n",
    "# create RNN network\n",
    "print(f'The model has {count_trainable_parameters(rnn_gen):,} trainable parameters')\n",
    "\n",
    "# select a criterion\n",
    "# create an optimiser\n",
    "\n",
    "# create our dataset\n",
    "# use a data loader\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "liveloss = PlotLosses()\n",
    "for epoch in range(n_epochs):\n",
    "    logs = {}\n",
    "\n",
    "    print(epoch,train_loss)\n",
    "\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6hW6QJ8ceTo"
   },
   "source": [
    "#### Let's try predicting with our network\n",
    "\n",
    "And, let's see how the network performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoQifyQuMqXT",
    "outputId": "73faacfa-c08e-4227-922a-6a96b0c0a85b"
   },
   "outputs": [],
   "source": [
    "# use the RNN to predict some text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3OilM4Xl7nS"
   },
   "source": [
    "##Implementing a Text \"Generator\" Network with LSTMs\n",
    "\n",
    "Simple RNNs have trouble learning. For example, if we try to predict the last word in the sentence: There are so many clouds in the sky. This is easy for a simple RNN to predict as the necessary context word clouds appeared just two words ago.\n",
    "\n",
    "However, look at the following example: I grew up in France... I speak fluent French. The distance between the contextual clue word France and the predicted word French could have been arbitrarily long in this text. Furthermore, the vanishing gradient and exploding gradient effects during backpropagation affect the performance of RNN. Given a very long sequence, information at the start of the sequence might have almost no impact at the end of the sequence.\n",
    "\n",
    "Let's re-implement our previous example using an LSTM instead of a vanilla RNN. to do this, we need to start by implementing the LSTM cell. This cell will update both its hidden state and cell state by using the different gates that we have seen in class:\n",
    "\n",
    "- Input gate\n",
    "- Forget gate\n",
    "- Output gate\n",
    "- Candidate update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8u0Ij-GlpNt"
   },
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(LSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        # we will streamline the implementation of the LSTM by combining the\n",
    "        # weights for all 4 operations (input gate, forget gate, output gate, candidate update)\n",
    "        # create a linear layer to map from input to hidden space\n",
    "        # create a linear layer to map from previous to current hidden space\n",
    "\n",
    "        # initialise the parameters\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "    def forward(self, input, h, c):\n",
    "        # apply the weights to both input and previous state\n",
    "\n",
    "        # separate the output into each of the LSTM operations\n",
    "\n",
    "        # apply the corresponding activations\n",
    "\n",
    "        # calculate the next cell state\n",
    "\n",
    "        # calculate the next hidden state\n",
    "\n",
    "        return h, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-GzMmaWqMwh"
   },
   "source": [
    "To construct a fully functional LSTM network, we create a similar `nn.Module` to the one used for the RNN, which will concatenate one or more LSTM cells and apply them to a given input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RK24y4Sk2U2"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, bias=False):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # create a list of modules\n",
    "\n",
    "        # create each layer in the network\n",
    "        # take care when defining the input size of the first vs later layers\n",
    "\n",
    "        # create a final linear layer from hidden state to network output\n",
    "\n",
    "    def init_hidden(self,  batch_size=1):\n",
    "        # initialise the hidden state and cell state\n",
    "\n",
    "    def forward(self, input, h0, c0):\n",
    "        # Input of shape (batch_size, seqence length , input_size)\n",
    "        # Output of shape (batch_size, output_size)\n",
    "\n",
    "        # iterate over all elements in the sequence\n",
    "            # iterate over each layer\n",
    "                # apply each layer\n",
    "                # take care to apply the layer to the input or the\n",
    "                # previous hidden state depending on the layer number\n",
    "\n",
    "                # store the hidden and cell state of each layer\n",
    "\n",
    "            # the hidden state of the last layer needs to be recorded\n",
    "            # to be used in the output\n",
    "\n",
    "        # calculate output for each element in the sequence\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxMbZCXWqd-B"
   },
   "source": [
    "We can now substitute the RNN for an LSTM in our test-generation network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgZABnoEfMIs"
   },
   "outputs": [],
   "source": [
    "class LSTM_GEN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, num_layers, num_unique_words):\n",
    "        super(LSTM_GEN, self).__init__()\n",
    "\n",
    "        # define your layers and activations\n",
    "        self.input_size = input_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_unique_words = num_unique_words\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # add a nn.Embedding\n",
    "        # add out LSTM\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # initialise hidden state\n",
    "\n",
    "        # store the word embeddings\n",
    "        # apply the LSTM\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbPUFwfvJhQ3"
   },
   "source": [
    "### The train and predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnVba569zWRC"
   },
   "outputs": [],
   "source": [
    "def train_lstm_gen(model, optimizer, criterion, dataloader):\n",
    "    # set model to train mode\n",
    "    # initialise the loss\n",
    "\n",
    "    # loop over dataset\n",
    "        # send data to device\n",
    "        # reset the gradients\n",
    "        # get output and hidden state\n",
    "\n",
    "        # compute the loss (change shape as crossentropy takes input as batch_size, number of classes, d1, d2, ...)\n",
    "\n",
    "        # backpropagate\n",
    "        # update weights\n",
    "\n",
    "    return train_loss/len(dataloader)\n",
    "\n",
    "\n",
    "def predict_lstm_gen(dataset, model, text, next_words=10):\n",
    "    # set model to evaluation mode\n",
    "\n",
    "    # loop over words\n",
    "        # take word from dataset and send to device\n",
    "        # compute output and hidden state\n",
    "\n",
    "        # take last output\n",
    "\n",
    "        # obtain probability vector for last output\n",
    "        # sample probability vector to get index in dataset\n",
    "\n",
    "        # get word corresponding to dataset\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EPn9jStJlrZ"
   },
   "source": [
    "### Hyperparameters, model initialisation and training loop (Afternoon exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 844
    },
    "id": "W4UJOe76zZaw",
    "outputId": "4b8f0b15-c0f4-4a65-ce01-9e932ae5277b"
   },
   "outputs": [],
   "source": [
    "device = device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = 128\n",
    "n_hidden = 128\n",
    "n_layers = 1\n",
    "embedding_dim = input_size\n",
    "n_unique_words = len(words_dataset.unique_words)\n",
    "batch_size = 128\n",
    "sequence_length = 4\n",
    "\n",
    "lr = 5e-3\n",
    "momentum = 0.5\n",
    "n_epochs = 20\n",
    "\n",
    "# create LSTM network\n",
    "print(f'The model has {count_trainable_parameters(lstm_gen):,} trainable parameters')\n",
    "\n",
    "# select a criterion\n",
    "# create an optimiser\n",
    "\n",
    "# create our dataset\n",
    "# use it with a data loader\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "liveloss = PlotLosses()\n",
    "for epoch in range(n_epochs):\n",
    "    logs = {}\n",
    "    # train the LSTM\n",
    "\n",
    "    print(epoch,train_loss)\n",
    "\n",
    "    logs['' + 'log loss'] = train_loss.item()\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxSXYr_wJzPs"
   },
   "source": [
    "Let's try to predict the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6ivX3ke9mUN",
    "outputId": "afb52505-8ba6-436c-8bca-b609063fa451"
   },
   "outputs": [],
   "source": [
    "# predict some text with the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKJEsHliHo61"
   },
   "source": [
    "We can see that the LSTM generates more coherent text, and jokes that are more funny too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8zTWHgJ66st"
   },
   "source": [
    "# Recent Advances\n",
    "\n",
    "The long-term memory in LSTM is [a specific instance](https://arxiv.org/pdf/1601.06733.pdf) of a more generic concept called _Attention_. The concept of Attention was introduced to solve one problem - when doing _Neural Machine Translation_ , the next word in the output sentence (in the output language) is not necessary related to the last (or second-to-last) word in the input sentence (in the input language). Since simple RNNs can only capture adjacency relationships, various styles of attention were tried to teach the model to look at a specific part of the input sentence in order to predict the next output word. [Many of these attention approaches](https://arxiv.org/abs/1409.0473) were successful and today far outperform LSTMs on the above tasks.\n",
    "\n",
    "![](https://github.com/acse-2020/ACSE-8/blob/main/implementation/practical_5/morning_lecture/images/self-attention.png?raw=1)\n",
    "\n",
    "One extremely successful kind of attention is _self attention_. Here, instead of mapping relationships between an output sequence and an input sequence, we map relationships between the different words of the same sentence. Going down this path, it was realised that the self-attention mechanism is more than just an add-on to RNNs and it might be possible to build entire networks out of self-attention alone. In [\"Attention is all you need\" (Vasuvani 2017)](https://arxiv.org/pdf/1706.03762.pdf) a neural network architecture called _Transformer_ was introduced that was composed entirely of self attention layers, and had some other innovations regarding memory.\n",
    "\n",
    "![](https://github.com/acse-2020/ACSE-8/blob/main/implementation/practical_5/morning_lecture/images/transformer.png?raw=1)\n",
    "\n",
    "In Feb 2019, a company called OpenAI introduced a variation of the transformer called GPT2 and [refused to release](https://slate.com/technology/2019/02/openai-gpt2-text-generating-algorithm-ai-dangerous.html) it _claiming it might destroy human society_ . This was a text generation model that could generate entire (_fake_) news articles from a one/few word prompt - think of it as autocomplete on steroids. They did eventually release it and is now available to try online: https://talktotransformer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lwWDk3i67bj"
   },
   "source": [
    "### In summary, we have learnt how to:\n",
    "- Implement a word-level text generator using RNNs\n",
    "- Implement word-level text generation with LSTMs, which outperforms vanilla RNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5X322hhJ2NV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
