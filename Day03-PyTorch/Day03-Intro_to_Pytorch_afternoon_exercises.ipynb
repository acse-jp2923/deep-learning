{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oHbTEGZ_rL0"
      },
      "source": [
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1DvKhAzLtk-Hilu7Le73WAOz2EBR5d41G\" width=\"400\"/>\n",
        "</p>\n",
        "\n",
        "---\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://pytorch.org/assets/images/pytorch-logo.png\" alt=\"drawing\" width=\"100\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "<h1 style=\"text-align: center;\"> Introduction to Pytorch for Deep Learning\n",
        "  – Exercises</h1>\n",
        "\n",
        "\n",
        "#### **Afternoon contents/agenda**\n",
        "\n",
        "1. Understanding the basics:\n",
        "- [But what is a convolution?](https://www.youtube.com/watch?v=KuXjwB4LzSA&ab_channel=3Blue1Brown)\n",
        "\n",
        "- [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk&t=1s&ab_channel=3Blue1Brown)\n",
        "\n",
        "- [What is backpropagation really doing?](https://www.youtube.com/watch?v=Ilg3gGewQ5U&t=2s&ab_channel=3Blue1Brown)\n",
        "\n",
        "2. In this exercise we will work with a chest x-ray dataset from [MedMnist](https://github.com/MedMNIST/MedMNIST) to tackle a reconstruction problem. Often, bio-engineering datasets have sparse or missing information which are difficult to to avoid due to poor design unexpected failures or restricitions in acquisition times. Interpolation is a common method to pre-process the data to simulate missing data, but fails when the amount of information is large. Here we will use a neural network to predict missing values by learning the distribution of the dataset as opposed to localised operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Pb9we5_rL1"
      },
      "source": [
        "### 2.0 Some imports and utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAsd9PdiApgK",
        "outputId": "281906f0-d534-45a9-c46f-b53e2c256724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXZ7QY4L_rL1",
        "outputId": "e031460b-c4ad-49d2-f377-76b341ad9b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda installed! Running on GPU 0 Tesla T4!\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary progressbar2 livelossplot monai -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from torchsummary import summary\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from livelossplot import PlotLosses\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
        "    torch.backends.cudnn.enabled   = True\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def set_device(device=\"cpu\", idx=0):\n",
        "    if device != \"cpu\":\n",
        "        if torch.cuda.device_count() > idx and torch.cuda.is_available():\n",
        "            print(\"Cuda installed! Running on GPU {} {}!\".format(idx, torch.cuda.get_device_name(idx)))\n",
        "            device=\"cuda:{}\".format(idx)\n",
        "        elif torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "            print(\"Cuda installed but only {} GPU(s) available! Running on GPU 0 {}!\".format(torch.cuda.device_count(), torch.cuda.get_device_name()))\n",
        "            device=\"cuda:0\"\n",
        "        else:\n",
        "            device=\"cpu\"\n",
        "            print(\"No GPU available! Running on CPU\")\n",
        "    return device\n",
        "\n",
        "device = set_device(\"cuda\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJEiTopU_rL2"
      },
      "source": [
        "### 2.1  Download and inspect the data using the commands below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-VOPyWQ_rL2",
        "outputId": "09e496c0-dc5a-45ad-d24b-e819702ad26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-30 15:15:18--  https://zenodo.org/record/6496656/files/chestmnist.npz\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.98.238, 188.185.79.172, 188.184.103.159, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.98.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
            "Location: /records/6496656/files/chestmnist.npz [following]\n",
            "--2023-11-30 15:15:19--  https://zenodo.org/records/6496656/files/chestmnist.npz\n",
            "Reusing existing connection to zenodo.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82802576 (79M) [application/octet-stream]\n",
            "Saving to: ‘chestmnist.npz.2’\n",
            "\n",
            "chestmnist.npz.2    100%[===================>]  78.97M  12.1MB/s    in 68s     \n",
            "\n",
            "2023-11-30 15:16:27 (1.16 MB/s) - ‘chestmnist.npz.2’ saved [82802576/82802576]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://zenodo.org/record/6496656/files/chestmnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7MTP18-_rL2",
        "outputId": "6c31de77-9fa5-47d7-c2e2-7c2ec9294484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_images', 'val_images', 'test_images', 'train_labels', 'val_labels', 'test_labels']\n"
          ]
        }
      ],
      "source": [
        "ex_data = np.load(\"./chestmnist.npz\")\n",
        "print(ex_data.files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCdhFXC4_rL2"
      },
      "source": [
        "### 2.3 Create a custom dataset\n",
        "\n",
        "* Create your own ``Dataset`` derived class that takes as initialisation arguments:\n",
        "  - ``data_path``, the path to the data\n",
        "  - a probability for a random mask ``p``,\n",
        "  - a ``transform`` to be applied to the data,\n",
        "  - and a ``split`` argument to dictate what part of the data to load (train, validation, test)\n",
        "\n",
        "* Load the data into an argument ``self.data`` inside the initialisation\n",
        "\n",
        "* Create a method for your class ``_get_mask``, that generates a binary mask of the size of the sample to randomly erase some data points based on the probability ``p``\n",
        "\n",
        "* Customise the  ``__getitem__`` class so that it loads a sample from ``self.data`` and returns a masked version of the sample, and the original sample (the former will be input to our network and the later the target)\n",
        "\n",
        "* Don't forget to set the built-in method ``__len__`` to the correct size\n",
        "\n",
        "* Instantiate the class for a training set and a validation set. Plot one input and output for each of these sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQaH3SGs6TlM",
        "outputId": "ba5ba498-963c-47ce-8f21-a194db332593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_images', 'val_images', 'test_images', 'train_labels', 'val_labels', 'test_labels']\n"
          ]
        }
      ],
      "source": [
        "print(ex_data.files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftuXB-Le6VTM",
        "outputId": "fa96c699-2a89-47ee-81e9-1118658785cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 1, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "ex_data[\"train_labels\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaMSI5ZS_I1W",
        "outputId": "e0958974-0f72-45cc-9eac-9bd53e746bb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ True, False,  True],\n",
              "       [ True, False, False],\n",
              "       [False,  True,  True]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "np.random.uniform(0, 1, size=(3,3)) > 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BN4muwLu_rL3"
      },
      "outputs": [],
      "source": [
        "class ChestMNIST(Dataset):\n",
        "    def __init__(self, data_path, split=\"train\", p=0.5, transform=None):\n",
        "      self.data_path = data_path\n",
        "      self.split = split\n",
        "      self.p = p\n",
        "      self.transform = transform\n",
        "      self.data = np.load(self.data_path)\n",
        "\n",
        "    def _get_mask(self, img_shape):\n",
        "      # Create a method for your class _get_mask, that generates a binary mask\n",
        "      # of the size of the sample to randomly erase some data points based on\n",
        "      # the probability p\n",
        "\n",
        "      mask = np.random.uniform(0, 1, size=(img_shape)) > self.p\n",
        "      return mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      # the data and the label (for a specific split)\n",
        "      return self.data[self.split + \"_images\"][idx], np.load(self.data_path)[self.split + \"_labels\"][idx]\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.data[self.split + \"_labels\"].shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "wHzIUiIIGfMv",
        "outputId": "f55472c0-e705-4c78-f9cc-226e63fac967"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x79506a369240>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(ChestMNIST(data_path=\"./chestmnist.npz\")._get_mask((13,13)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Z1TfPg8K_rL3"
      },
      "outputs": [],
      "source": [
        "## Instantiate datasets\n",
        "# our_data = ChestMNIST(\n",
        "#     data_path=\"./chestmnist.npz\",\n",
        "#     split=\"train\",\n",
        "#     p=0.5,\n",
        "#     transform=None\n",
        "# )\n",
        "\n",
        "## Plots\n",
        "# print(our_data.__len__())\n",
        "# our_data.__getitem__(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ew068BZ_rL3"
      },
      "source": [
        "### 2.4 Modify our ``simpleFFN`` model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHSUsJap_rL3"
      },
      "source": [
        "* Add two more hidden layers to the model\n",
        "\n",
        "* Change the size of the output to match the size of the input\n",
        "\n",
        "* Change the activation of the model to [``Mish``](https://arxiv.org/abs/1908.08681)\n",
        "\n",
        "* Change the activation of last layer, what should it be?\n",
        "\n",
        "* Instantiate the model and print a summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_LkxXZQc_rL3"
      },
      "outputs": [],
      "source": [
        "# Modify model\n",
        "class simpleFFN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size_1=100, hidden_size_2=50, output_size=10):\n",
        "    super(simpleFFN, self).__init__()\n",
        "    self.hidden_1 = nn.Linear(input_size, hidden_size_1, bias=False)\n",
        "    self.hidden_2 = nn.Linear(hidden_size_1, hidden_size_2, bias=False)\n",
        "    self.output = nn.Linear(hidden_size_2, 10, bias=False)\n",
        "    self.final_activation = nn.Sigmoid()\n",
        "    self.activation = nn.Mish()\n",
        "\n",
        "  def forward(self, X):\n",
        "    z1 = self.hidden_1(X.flatten(start_dim=1))\n",
        "    a1 = self.activation(z1)\n",
        "    z2 = self.hidden_2(a1)\n",
        "    a2 = self.activation(z2)\n",
        "    z3 = self.output(a2)\n",
        "    a3 = self.final_activation(z3)\n",
        "    return a3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EofNzK3Z_rL4"
      },
      "outputs": [],
      "source": [
        "# Instantiate and print summary\n",
        "model = simpleFFN(input_size=(28*28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbhSounjAHmG",
        "outputId": "e8b4bc68-894c-467b-b26b-6d21f39d37d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "simpleFFN(\n",
              "  (hidden_1): Linear(in_features=784, out_features=100, bias=False)\n",
              "  (hidden_2): Linear(in_features=100, out_features=50, bias=False)\n",
              "  (output): Linear(in_features=50, out_features=10, bias=False)\n",
              "  (final_activation): Sigmoid()\n",
              "  (activation): Mish()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inc4U5fG_rL4"
      },
      "source": [
        "### 2.5 Prepare parameters and hyperparameters for training\n",
        "\n",
        "* Set your hyperparameters:\n",
        "    - seed: 42\n",
        "    - mask probability: 0.6 (this is a heavy damaged imputation problem! We are only keeping 60% of the information)\n",
        "    - learning rate: 1e-2\n",
        "    - weight decay = 1e-6 (applied in optimiser)\n",
        "    - batch size: 128\n",
        "    - number of epochs: 30\n",
        "\n",
        "\n",
        "* Instantiate ``simpleFFN`` as our model with hidden sizes: 150, 50, 50, 150\n",
        "\n",
        "* Instantiate ``Adam`` as the optimiser\n",
        "\n",
        "* Instantiate ``MSELoss`` as a criterion\n",
        "\n",
        "* Collect any list of transformations you think are appropriate for this problem\n",
        "\n",
        "* Instantiate the training and validation dataset and create the dataloader for each\n",
        "\n",
        "* Visualise an input and target batch using ``make_grid``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lpavKdj9KaAa"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "h-p2-TWEJCpk"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "set_seed(42)\n",
        "mask_probability = 0.6 # (this is a heavy damaged imputation problem! We are only keeping 60% of the information)\n",
        "learning_rate = 1e-2\n",
        "weight_decay = 1e-6 # (applied in optimiser)\n",
        "batch_size = 128\n",
        "number_of_epochs = 30\n",
        "\n",
        "# Training set up: model, optimiser, criterion\n",
        "model = simpleFFN(input_size=(28*28))\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "from torchvision import transforms\n",
        "# transform = Compose([ToTensor(),]) # Compose a list of transformations\n",
        "\n",
        "\n",
        "# Transforms, Dataset and dataloader\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2pewVAL9_rL4"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = ChestMNIST(\n",
        "    data_path=\"./chestmnist.npz\",\n",
        "    split=\"train\",\n",
        "    p=mask_probability,\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "val_dataset = ChestMNIST(\n",
        "    data_path=\"./chestmnist.npz\",\n",
        "    split=\"val\",\n",
        "    p=mask_probability,\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "test_dataset = ChestMNIST(\n",
        "    data_path=\"./chestmnist.npz\",\n",
        "    split=\"test\",\n",
        "    p=mask_probability,\n",
        "    transform=transforms\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    pin_memory=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    pin_memory=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    pin_memory=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdAu4YFSCTzC"
      },
      "outputs": [],
      "source": [
        "# Visualise a batch sample\n",
        "train_iterator = iter(train_loader)\n",
        "train_batch = next(train_iterator)\n",
        "# print(train_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1VzFokOOsUd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ3DlidKMGva"
      },
      "outputs": [],
      "source": [
        "dimensions = [784, 200, 50, 10]\n",
        "\n",
        "class simpleFFN(nn.Module):\n",
        "    def __init__(self, dimensions):\n",
        "        super(simpleFFN, self).__init__()\n",
        "        # fully connected\n",
        "        self.fc1 = nn.Linear(dimensions[0], dimensions[1], bias=False)\n",
        "        self.fc2 = nn.Linear(dimensions[1], dimensions[2], bias=False)\n",
        "        self.fc3 = nn.Linear(dimensions[2], dimensions[-1], bias=False)\n",
        "        # activation functions/special\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # out = self.fc1(X.float())\n",
        "        out = self.fc1(X.reshape(-1, 28*28).float())\n",
        "        out = self.tanh(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.tanh(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "model = simpleFFN(dimensions)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())   # instantiate the optimizer\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "8VcBP-8vpEv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufNOcBVs_rL4"
      },
      "source": [
        "### 2.6 Modify training and validation functions\n",
        "\n",
        "* Make the necessary modifications to the ``train`` and ``valid`` functions from the lecture to adapt to our reconstruction problem\n",
        "\n",
        "* Does prediction play a role in this problem?\n",
        "\n",
        "* Is accuracy a suitable metric?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTxUm11yQcrs"
      },
      "outputs": [],
      "source": [
        "# Modify functions\n",
        "def train(model, optimizer, criterion, data_loader):\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "    print(\"training mode\")\n",
        "    train_loss, train_accuracy = 0, 0\n",
        "    print(\"entering loop\")\n",
        "    for input, target in data_loader:\n",
        "        print(0)\n",
        "        input, target = input.to(device), target.to(device)\n",
        "        print(\"1\")\n",
        "        optimizer.zero_grad()\n",
        "        print(\"2\")\n",
        "        print(input.shape)\n",
        "        output = model(input)\n",
        "        transformed_output = torch.nn.functional.one_hot(preds.max(axis=1)[1])\n",
        "        print(\"3\")\n",
        "        loss = criterion(transformed_output, target)\n",
        "        print(\"4\")\n",
        "        loss.backward()\n",
        "        train_loss += loss*input.size(0)\n",
        "        pred = output.softmax(dim=1).max(dim=1)[1]\n",
        "        train_accuracy += accuracy_score(target.cpu().numpy(), pred.detach().cpu().numpy())*input.size(0)\n",
        "        optimizer.step()\n",
        "    print(\"b\")\n",
        "    train_loss = train_loss / len(data_loader.dataset)\n",
        "    train_accuracy = train_accuracy/len(data_loader.dataset)\n",
        "    return train_loss, train_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VZ0-VjbfltI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxSrHUMwQeaW"
      },
      "outputs": [],
      "source": [
        "train(model=model,\n",
        "      optimizer=optimizer,\n",
        "      criterion=loss_func,\n",
        "      data_loader=train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_batch[0]\n",
        "y = train_batch[1]"
      ],
      "metadata": {
        "id": "qb0GcFkYlofx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVodOmh_hW1C"
      },
      "outputs": [],
      "source": [
        "# model(torch.Tensor(X.reshape(-1, 28*28)).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(torch.Tensor(X.reshape(-1, 28*28)).to(device))\n",
        "preds"
      ],
      "metadata": {
        "id": "gSJQtnAEqgFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFjjr3FAhWEj"
      },
      "outputs": [],
      "source": [
        "# preds = torch.nn.functional.one_hot(preds.max(axis=1)[1])\n",
        "# preds.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.argmax(axis=1)"
      ],
      "metadata": {
        "id": "oPoF3ZSbuZ11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.CrossEntropyLoss(preds, y.argmax(axis=1))"
      ],
      "metadata": {
        "id": "it_Eg2hotCQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4ofsc2mhUlC"
      },
      "outputs": [],
      "source": [
        ":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GXp7Hgi_rL4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def valid(model, criterion, data_loader):\n",
        "    \" Equivalent to the training function without any backpropagation or optimisation steps\"\n",
        "    model.eval()\n",
        "    valid_loss, valid_accuracy = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for input, target in data_loader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "\n",
        "            output = model(input)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            valid_loss += loss*input.size(0)\n",
        "\n",
        "            pred = output.softmax(dim=1).max(dim=1)[1]\n",
        "\n",
        "            valid_accuracy += accuracy_score(target.cpu().numpy(), pred.detach().cpu().numpy())*input.size(0)\n",
        "\n",
        "        valid_loss = valid_loss / len(data_loader.dataset)\n",
        "        valid_accuracy = valid_accuracy/len(data_loader.dataset)\n",
        "        return valid_loss, valid_accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMiovebI_rL4"
      },
      "source": [
        "### 2.7 Train and validate the model\n",
        "\n",
        "* Train your model\n",
        "\n",
        "* Visualise the output of a validation sample along training\n",
        "\n",
        "* At the end of training, plot the 32 reconstructed and target samples from a validation batch\n",
        "\n",
        "* What do you observe?\n",
        "\n",
        "* Are the results as expected?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s82at41L_rL4"
      },
      "outputs": [],
      "source": [
        "# Train model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ1EnssDjJs1"
      },
      "outputs": [],
      "source": [
        "# Plot recon and target from valid batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH8LPdae_rL4"
      },
      "source": [
        "### 2.8 Save model to disk and load\n",
        "\n",
        "* ``Pytorch`` stores all the parameters of models and optimizers, their weights and biases in an easy to read dictionary called a \"state-dict\".\n",
        "\n",
        "* When we store models and optimizers, we store the state-dict.  \n",
        "\n",
        "* Together with the model definition we can then restore the model to it's state when we stored it to disk.\n",
        "\n",
        "* Let's look at the contents of the state-dict of both our optimizer and our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-xx9IMh_rL4"
      },
      "outputs": [],
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimiser's state_dict\n",
        "print(\"Optimiser's state_dict:\")\n",
        "for var_name in optimiser.state_dict():\n",
        "    print(var_name, \"\\t\", optimiser.state_dict()[var_name])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTJyPrbV_rL5"
      },
      "source": [
        "From colab (and locally) we can store models to disk using ```torch.save``` and passing both a models state_dict() and a path where to store it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0UVJSmK_rL5"
      },
      "outputs": [],
      "source": [
        "#!mkdir '/content/gdrive/My Drive/models'  ## create the director for storing the model in Google Drive\n",
        "\n",
        "model_save_name = 'chestmnist_simpleFFN_model.pt'           # .pt and .pth are common file extensions for saving models in pytorch\n",
        "path = F\"/content/gdrive/My Drive/models/{model_save_name}\" # use this to store in your Google Drive storage\n",
        "path = F'./{model_save_name}'                               # use this to store locally (it will be erased once the colab session is over)\n",
        "torch.save(model.state_dict(), path)\n",
        "\n",
        "optimiser_save_name = 'chestmnist_simpleFFN_optimiser.pt'\n",
        "path = F\"/content/gdrive/My Drive/models/{optimiser_save_name}\"\n",
        "path = F\"./{optimiser_save_name}\"\n",
        "torch.save(model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz4ZRrXq_rL5"
      },
      "source": [
        "Finally, we can restore models from the saved ```state_dict```'s and do a number of things such as:\n",
        "1. Use it as a checkpoint and continue training (given we stored the optimizer as well)\n",
        "2. Make predictions from our model\n",
        "3. Perform inspections of our model\n",
        "4. Use our model in ensembles\n",
        "5. ...\n",
        "\n",
        "By default a loaded model is put into ```.train()``` mode. So be careful when using networks that behave different depending on training and test time e.g. dropout regularized networks or batch-normalized networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG_13t8i_rL5"
      },
      "outputs": [],
      "source": [
        "model_save_name = 'chestmnist_simpleFFN_model.pt'           # .pt and .pth are common file extensions for saving models in pytorch\n",
        "path = F\"/content/gdrive/My Drive/models/{model_save_name}\" # use this to store in your Google Drive storage\n",
        "path = F'./{model_save_name}'\n",
        "\n",
        "model = simpleFFN(input_size= 1 * 28 * 28, hidden_size_1=150, hidden_size_2=50, hidden_size_3=50, hidden_size_4=150).to(device) ## creates an instance of the model\n",
        "model.load_state_dict(torch.load(path)) ## loads the parameters of the model in path. state_dict is a dictionary object that maps each layer in the model to its trainable parameters (weights and biases).\n",
        "model.eval()\n",
        "\n",
        "valid_loss = valid(model, mseloss, valid_loader)\n",
        "print(\"Avg. Valid Loss: %1.3f\" % valid_loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uADu0Udo_rL5"
      },
      "source": [
        "### 2.8 Training with Unet\n",
        "\n",
        "* Instantiate a ``U-net`` using the snipped below.\n",
        "\n",
        "* For now you do not need to understand what a ``U-net`` is or how it works. This will be explored later in the course.\n",
        "\n",
        "* Print the summary of the model and have a look at what kind of layers it includes. Search for these layers in the ``Pytorch`` documentation to gain a general understanding of their operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRRtG8aM_rL5"
      },
      "outputs": [],
      "source": [
        "from monai.networks.nets import UNet\n",
        "set_seed(42)\n",
        "model = UNet(spatial_dims=2, in_channels=1, out_channels=1, channels=(8, 8, 8), strides=(1, 1,), act=\"mish\").to(device)\n",
        "summ = summary(model, input_size=(1, 28, 28))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvzwwRlWhwFl"
      },
      "source": [
        "* Train the model with the same hyperparameters from before. Don't forget to re-initialise the optimiser with the correct model parameters\n",
        "\n",
        "* As before, visualise some validation samples along training.\n",
        "\n",
        "* Plot 32 reconstructed and target samples from the validation batch\n",
        "\n",
        "* Save your final model\n",
        "\n",
        "* What differences do you observe from training with a simple feed-forward network? Why do you think that is?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekM5VEbsmn-r"
      },
      "outputs": [],
      "source": [
        "# Instantiate optimiser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyZggtJq_rL5"
      },
      "outputs": [],
      "source": [
        "# Train model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqzSyfiRjMT3"
      },
      "outputs": [],
      "source": [
        "# Plot recon and target from valid batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Jg5novaU7b0"
      },
      "outputs": [],
      "source": [
        "# Save model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('mltorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b996dd704eec390d394e4eff83c8c73a7c6e40d054c583352c9aa3265aab441b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}