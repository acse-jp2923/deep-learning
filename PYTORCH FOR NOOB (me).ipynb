{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05616e2",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480dd3c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "!pip install pycm livelossplot\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from livelossplot import PlotLosses\n",
    "from pycm import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e09e6",
   "metadata": {},
   "source": [
    "### Setting the seed and setting up cuda/gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ## uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4520ad",
   "metadata": {},
   "source": [
    "## CUSTOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525f74b",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ChestMNIST(Dataset):\n",
    "    def __init__(self, data_path, split=\"train\", p=0.2, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.split = split\n",
    "        self.p = p\n",
    "        self.transform = transform\n",
    "        if self.split == \"train\":\n",
    "            self.data = np.load(self.data_path)[\"train_images\"]        \n",
    "        elif self.split == \"test\":\n",
    "            self.data = np.load(self.data_path)[\"test_images\"]        \n",
    "        elif self.split == \"val\":\n",
    "            self.data = np.load(self.data_path)[\"val_images\"]\n",
    "        else:\n",
    "            return ValueError(\"wrong split\")\n",
    "            \n",
    "    def _get_mask(self, img_shape):\n",
    "        # Create a method for your class _get_mask, that generates a binary mask\n",
    "        # of the size of the sample to randomly erase some data points based on\n",
    "        # the probability p\n",
    "\n",
    "        mask = np.random.uniform(0, 1, size=(img_shape)) > self.p\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        # masked image (want to pred), actual image (our target)\n",
    "        return sample * self._get_mask(sample.shape), sample\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e111b4",
   "metadata": {},
   "source": [
    "## get a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167bffa",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(mnist_train.data, mnist_train.targets)\n",
    "# gives you n, n-1 random ids that correspond to test and train subsets\n",
    "# n is the train proportion \n",
    "train_idxs, valid_idxs = [(train_idx, valid_idx) for train_idx, valid_idx in shuffler][0]\n",
    "\n",
    "X_train, y_train = mnist_train.data[train_idxs], mnist_train.targets[train_idxs]\n",
    "X_val, y_val =     mnist_train.data[valid_idxs], mnist_train.targets[valid_idxs]\n",
    "X_test, y_test =   mnist_test.data, mnist_test.targets\n",
    "\n",
    "# NOTE torch categorical data = .long()\n",
    "mnist_train =    TensorDataset(X_train, y_train.long())\n",
    "mnist_validate = TensorDataset(X_val, y_val.long())\n",
    "mnist_test =     TensorDataset(X_test, y_test.long())\n",
    "\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(mnist_train.data, mnist_train.targets)\n",
    "# # gives you n, n-1 random ids that correspond to test and train subsets\n",
    "# # n is the train proportion \n",
    "# train_idxs, valid_idxs = [(train_idx, valid_idx) for train_idx, valid_idx in shuffler][0]\n",
    "# X_train, y_train = apply_standardization(mnist_train.data[train_idxs].float()), mnist_train.targets[train_idxs]\n",
    "# X_val, y_val =     apply_standardization(mnist_train.data[valid_idxs].float()), mnist_train.targets[valid_idxs]\n",
    "# X_test, y_test =   apply_standardization(mnist_test.data.float()), mnist_test.targets\n",
    "# mnist_train =    TensorDataset(X_train, y_train.long())\n",
    "# mnist_validate = TensorDataset(X_val, y_val.long())\n",
    "# mnist_test =     TensorDataset(X_test, y_test.long())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebda16",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.1307], std=[0.3081]),\n",
    "])\n",
    "\n",
    "validation_test_transform = Compose([\n",
    "    Normalize(mean=[0.1307], std=[0.3081])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b832156",
   "metadata": {},
   "source": [
    "### Conf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ec1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ConfusionMatrixDisplay.from_predictions(y_gt, y_pred, ax=ax, colorbar=False, cmap='bone_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bca5b1",
   "metadata": {},
   "source": [
    "# DATALOADERS (AND GETTING TORCH DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca376994",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "# train_dataset = ChestMNIST(\n",
    "#     data_path=\"./chestmnist.npz\",\n",
    "#     split=\"train\",\n",
    "#     p=mask_probability,\n",
    "#     transform=transformations\n",
    "# )\n",
    "#\n",
    "# val_dataset = ChestMNIST(\n",
    "#     data_path=\"./chestmnist.npz\",\n",
    "#     split=\"val\",\n",
    "#     p=mask_probability,\n",
    "#     transform=transformations\n",
    "# )\n",
    "#\n",
    "# test_dataset = ChestMNIST(\n",
    "#     data_path=\"./chestmnist.npz\",\n",
    "#     split=\"test\",\n",
    "#     p=mask_probability,\n",
    "#     transform=transformations\n",
    "# )\n",
    "\n",
    "# datasets will be torch tensors eg torch.TensorDataset(X, y) X=SampleInput, y=SampleLabel\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e707e",
   "metadata": {},
   "source": [
    "# AN EXAMPLE NEURAL NETWORK IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea929b8",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "dimensions = [784, 150, 50, 50, 150, 784]\n",
    "\n",
    "class neural_net(nn.Module):\n",
    "    def __init__(self, dimensions):\n",
    "        super(neural_net, self).__init__()\n",
    "        # fully connected\n",
    "        self.fc1 = nn.Linear(dimensions[0], dimensions[1], bias=False)\n",
    "        self.fc2 = nn.Linear(dimensions[1], dimensions[2], bias=False)\n",
    "        self.fc3 = nn.Linear(dimensions[2], dimensions[3], bias=False)\n",
    "        self.fc4 = nn.Linear(dimensions[3], dimensions[4], bias=False)\n",
    "        self.fc5 = nn.Linear(dimensions[4], dimensions[-1], bias=False)\n",
    "        # activation functions/special\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mish = nn.Mish()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # INPUT LAYER\n",
    "        out = self.fc1(X.flatten(start_dim=1).float())\n",
    "        out = self.mish(out)\n",
    "        \n",
    "        # HIDDEN 1\n",
    "        out = self.fc2(out)\n",
    "        out = self.mish(out)\n",
    "        \n",
    "        # HIDDEN 2\n",
    "        out = self.fc3(out)\n",
    "        out = self.mish(out)\n",
    "\n",
    "        # HIDDEN 3\n",
    "        out = self.fc4(out)\n",
    "        out = self.mish(out)\n",
    "        \n",
    "        # OUTPUT LAYER\n",
    "        out = self.fc5(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        # TRANSFORMED OUTPUT LAYER \n",
    "        return out.view(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a373f0",
   "metadata": {},
   "source": [
    "# CHECKING A BATCH (+ OPT/LOSS FUNCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0e695",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "#~ CHECKING EVERYTHING FITS CELL\n",
    "\n",
    "# get a batch\n",
    "train_batch = next(iter(train_loader))\n",
    "\n",
    "X = torch.Tensor(train_batch[0])\n",
    "y = torch.Tensor(train_batch[1])\n",
    "\n",
    "print(f\"X.shape = {X.shape}\")\n",
    "print(f\"y.shape = {y.shape}\")\n",
    "\n",
    "# ensure prediction works\n",
    "model = neural_network()\n",
    "output = model(X)\n",
    "\n",
    "print(f\"  OUTPUT SHAPE: {output.shape} (pre-softmax)\")\n",
    "print(f\"EXPECTED SHAPE: {y.shape}\")\n",
    "\n",
    "# ensure loss works\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(output, y))\n",
    "\n",
    "# --\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay, lr=learning_rate)   # instantiate the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# # get a batch\n",
    "# train_batch = next(iter(train_loader))\n",
    "\n",
    "# # masked images\n",
    "# X = torch.Tensor(train_batch[0]).reshape(-1,1,28,28)\n",
    "# # unmasked images\n",
    "# y = torch.Tensor(train_batch[1].float())\n",
    "\n",
    "# print(f\"X.shape = {X.shape}\")\n",
    "# print(f\"y.shape = {y.shape}\")\n",
    "# # ensure prediction works\n",
    "# model = LeNet5drbn()\n",
    "# output = model(X)\n",
    "\n",
    "# print(f\"  OUTPUT SHAPE: {output.shape}\")\n",
    "# print(f\"EXPECTED SHAPE: {y.shape}\")\n",
    "\n",
    "# # ensure loss works\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# y = y.long()\n",
    "# print(criterion(output, y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe00c56",
   "metadata": {},
   "source": [
    "# TRAIN, VALIDATE, EVALUATE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ef069",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def train(model, optimizer, criterion, data_loader):\n",
    "    model.train()##\n",
    "    ### set the model to train\n",
    "    model.to(device)\n",
    "    train_loss, train_accuracy = 0, 0\n",
    "    for X, y in data_loader:\n",
    "        \n",
    "        X, y = X.to(device), y.to(device)\n",
    "#         print(X.view(-1, 1, 28, 28).shape)\n",
    "#         X = X.view(-1, 1, 28, 28)\n",
    "#         outputs = model(X)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X.view(-1, 1, 28, 28))\n",
    "        \n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()##\n",
    "        train_loss += loss*y.size(0)\n",
    "        y_pred = F.log_softmax(outputs, dim=1).max(1)[1]##\n",
    "        train_accuracy += accuracy_score(\n",
    "            y.cpu().numpy(), \n",
    "            y_pred.detach().cpu().numpy())*X.size(0) ##\n",
    "\n",
    "        optimizer.step()\n",
    "        ### your code goes here\n",
    "    \n",
    "    # loss, acc\n",
    "    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)\n",
    "\n",
    "\n",
    "def validate(model, criterion, data_loader):\n",
    "    ### set the model to evaluate\n",
    "    validation_loss, validation_accuracy = 0, 0\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X.view(-1, 1, 28, 28))\n",
    "            loss = criterion(outputs, y)\n",
    "            validation_loss += loss*y.size(0)\n",
    "            y_pred = F.log_softmax(outputs, dim=1).max(1)[1]##\n",
    "            validation_accuracy += accuracy_score(\n",
    "                y.cpu().numpy(), \n",
    "                y_pred.detach().cpu().numpy())*X.size(0) ##\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    ### set the model to evaluate\n",
    "    ys, y_preds = [], []\n",
    "    model.to(device)\n",
    "    for X, y in data_loader:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X.view(-1, 1, 28, 28))\n",
    "            y_pred = F.log_softmax(outputs, dim=1).max(1)[1]\n",
    "            y_preds.append(y_pred.cpu().numpy())\n",
    "            ys.append(y.cpu().numpy())\n",
    "\n",
    "\n",
    "    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02052cd",
   "metadata": {},
   "source": [
    "# TRAINING LOOP (Live plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7774778",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(model, model_params=None):\n",
    "    set_seed(seed)\n",
    "    model = model(model).to(device)\n",
    "\n",
    "    optimizer = optimizer = torch.optim.Adam(model.parameters(), lr=lr,momentum=momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        mnist_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=False, \n",
    "        num_workers=0,)\n",
    "    \n",
    "    validation_loader = DataLoader(\n",
    "        mnist_validate,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=False, \n",
    "        num_workers=0,)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        mnist_test,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=False, \n",
    "        num_workers=0,)\n",
    "\n",
    "    liveloss = PlotLosses()\n",
    "    for epoch in range(30):\n",
    "        logs = {}\n",
    "        train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
    "\n",
    "        logs['' + 'log loss'] = train_loss.item()\n",
    "        logs['' + 'accuracy'] = train_accuracy.item()\n",
    "\n",
    "        validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n",
    "        logs['val_' + 'log loss'] = validation_loss.item()\n",
    "        logs['val_' + 'accuracy'] = validation_accuracy.item()\n",
    "\n",
    "        liveloss.update(logs)\n",
    "        liveloss.draw()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccafe9",
   "metadata": {},
   "source": [
    "## Transfer learning!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783da58",
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "# uses 2 helper functions\n",
    "def set_parameter_requires_grad(model, requires_grad=False):\n",
    "    \"\"\"https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = requires_grad\n",
    "    return None\n",
    "def get_params_to_update(model):\n",
    "    \"\"\" Returns list of model parameters that have required_grad=True\"\"\"\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "    return params_to_update\n",
    "\n",
    "\n",
    "\n",
    "model = models.resnet18(pretrained=True).to(device)\n",
    "set_parameter_requires_grad(model, False)### use the provdied set_parameter_requires_grad to disable training\n",
    "print(model)\n",
    "# look for the name of the output layer, in this case its fc\n",
    "# that layer to be one that fits your models use case :)\n",
    "# the below example is converting a binary-classification resnet thing.\n",
    "model.fc = nn.Linear(model.fc.in_features, 2).to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90790235",
   "metadata": {},
   "source": [
    "## Other stuff :/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b76a4",
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "### Saving a model\n",
    "# model_save_name = \n",
    "path = f\"/content/gdrive/My Drive/models/{model_save_name}\"\n",
    "torch.save(model.state_dict(), path)\n",
    "\n",
    "### Loading a model\n",
    "\n",
    "### output size for conv layer\n",
    "def calculate_conv_output_size(input_size, kernel_size, stride, padding, num_filters):\n",
    "    https://chat.openai.com/share/132dd59a-df37-4c72-adc4-89cae275a6c6\n",
    "    \n",
    "    # Extract input size dimensions\n",
    "    input_height, input_width = input_size\n",
    "\n",
    "    # Extract kernel size dimensions\n",
    "    kernel_height, kernel_width = kernel_size\n",
    "\n",
    "    # Calculate the output height and width\n",
    "    output_height = ((input_height + 2 * padding - kernel_height) // stride) + 1\n",
    "    output_width = ((input_width + 2 * padding - kernel_width) // stride) + 1\n",
    "\n",
    "    return (output_height, output_width)\n",
    "\n",
    "# Example usage:\n",
    "input_size = (32, 32)\n",
    "kernel_size = (3, 3)\n",
    "stride = 1\n",
    "padding = 1\n",
    "num_filters = 64\n",
    "\n",
    "output_size = calculate_conv_output_size(input_size, kernel_size, stride, padding, num_filters)\n",
    "print(\"Output size:\", output_size)\n",
    "\n",
    "### imagestandardization\n",
    "\n",
    "def apply_standardization(X, mean=0.1307, std=0.3081): # define an standardisation function\n",
    "#     px_range = 225. \n",
    "    X /= 255.\n",
    "    return (X - mean) / std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you have a dataset class you can use \n",
    "def show_batch(dataset, nr=4, nc=4):\n",
    "    # this function comes from lecture 5: (VAEs)\n",
    "    fig, axarr = plt.subplots(nr, nc, figsize=(10, 10))\n",
    "    for i in range(nr):\n",
    "    for j in range(nc):\n",
    "        idx = random.randint(0, len(train_ds))\n",
    "        sample, target = train_ds[idx]\n",
    "        try:\n",
    "            axarr[i][j].imshow(sample) # if PIL\n",
    "        except:\n",
    "            axarr[i][j].imshow(sample.permute(1,2,0)) # if tensor of shape CHW\n",
    "            target_name = train_ds.classes[target]\n",
    "            axarr[i][j].set_title(\"%s (%i)\"%(target_name, target))\n",
    "\n",
    "    fig.tight_layout(pad=1.5)\n",
    "    plt.show()\n",
    "    \n",
    "show_batch(train_ds, 5, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
